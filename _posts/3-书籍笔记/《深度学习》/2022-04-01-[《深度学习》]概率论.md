---
title: 【《深度学习》】概率与信息论
date: 2022-04-01 07:00:00 +/-0800
categories: [书籍笔记,《深 度 学 习》]
tags: [《深 度 学 习》]     # TAG names should always be lowercase 标记名称应始终为小写
---
# 概率论

概率论是研究随机性和不确定性的科学，在人工智能领域主要有两大应用：
1. 概率论可以指导人工智能模型所需模拟或近似的概率分布。
2. 概率与统计可以帮助我们分析模型的预测的准确性。

可以说，线性代数和概率论是人工智能系统的两大数学基石，这一章对于理解很多机器学习模型的背后驱动力是关键的。

## 概率

许多人，尤其是受了很多理工科教育的人都对世界持机械决定论的观点：只要给定初始条件，宇宙万物走势都可以确定性的运行，不存在任何随机性。例如爱因斯坦曾经说过：上帝不会掷骰子。但是实际上很多因素造成了我们的系统往往是不确定的，例如在数学模型中，主要因素有三：

1. 我们需要模拟的系统本身是不确定的。比如量子力学认为宇宙中的基本粒子本质上是概率波，我们想用抛去任何随机性的模型来模拟这个系统，常常会产生很大的偏差。

2. 观测的不确定性。即使我们需要模拟的对象是确定的，但是由于我们观测方法的限制，我们常常无法观测对系统造成影响的所有的变量。

3. 模型的不确定性。通常我们的模型不可能涵盖所有的信息，常常要舍弃某些信息，例如我们想用机器人预测周边所有物体的运动轨迹，通常我们需要将空间离散成一些更小的空间单元来预测这些物体会出现在哪些单元，但是这些单元也无法划分成无限小的，物体可能出现在单元中的任意位置，这一限制就造成了模型本身的不确定性。


概率论就是用来研究这种随机性的过程。传统上，概率论更多的是用来分析事件发生的频率，这就要求事件是可重复的，比如我们可以多次掷骰子，这是<b>频率统计(frequentist statistics)</b>观点。但是现实中，很多事情是无法重复的，比如医生要分析病人是否患感冒的概率，我们无法将病人克隆许多次，并且保证他们具有完全相同的症状。这种情况下，我们常常用概率表示某种置信度(degree of belief)，比如1是表示我们100%确定病人得了感冒，而0是我们100%确定病人没有得感冒，这种用概率来表示置信度的观点被称作<b>贝叶斯统计(Bayesian statistics)</b>。

- <b>频率派概率（frequentist probability）</b>: 直接与事件发生的频率相联系，被称为。
- <b>贝叶斯概率（Bayesian probability）</b>: 涉及到确定性水平。

## 随机变量

首先需要理解的概念是<b>随机变量(random variable)</b>，顾名思义，它就是某一变量可能随机的取不同数值。。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。随机变量可以是离散的也可以是连续的。


## 概率分布

<b>概率分布（probability distribution）</b>用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散的还是连续的。

### 离散型变量和概率质量函数

离散型变量的概率分布可以用<b>概率质量函数（probability mass function, PMF）[也将它翻译成概率分布律]</b>来描述。我们通常用大写字母 <font face="Times New Roman"><b><i> P </i></b></font> 来表示概率质量函数。通常每一个随机变量都会有一个不同的概率质量函数，并且读者必须根据随机变量来推断所使用的PMF，而不是根据函数的名称来推断；例如，<font face="Times New Roman"><b><i> P(x) </i></b></font> 通常和 <font face="Times New Roman"><b><i> P(y) </i></b></font> 不一样。

概率质量函数将随机变量能够取得的每个状态映射到随机变量取得该状态的概
率。<font face="Times New Roman"><b><i> X = x </i></b></font>  的概率用 <font face="Times New Roman"><b><i> P(x) </i></b></font> 来表示，概率为 1 表示 <font face="Times New Roman"><b><i> X = x </i></b></font>  是确定的，概率为 0 表示 <font face="Times New Roman"><b><i> X = x </i></b></font>  是不可能发生的。有时为了使得PMF的使用不相互混淆，我们会明确写出随机变量的名称：<font face="Times New Roman"><b><i> P ( X = x) </i></b></font>。有时我们会先定义一个随机变量，然后用 <font face="Times New Roman"><b><i> ~ </i></b></font> 符号来说明它遵循的分布：<font face="Times New Roman"><b><i> x ~ P(x) </i></b></font>。

概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称为 <b>联合概率分布（joint probability distribution）</b>。<font face="Times New Roman"><b><i> P(X = x; Y = y) </i></b></font> 表示 <font face="Times New Roman"><b><i> X = x  </i></b></font>和 <font face="Times New Roman"><b><i> Y = y  </i></b></font> 同时发生的概率。我们也可以简写为 <font face="Times New Roman"><b><i> P(x , y)  </i></b></font>。

如果一个函数 <font face="Times New Roman"><b><i> P </i></b></font> 是随机变量 <font face="Times New Roman"><b><i> x </i></b></font>  的PMF，必须满足下面这几个条件：

-  在<font face="Times New Roman"><b><i> P </i></b></font> 的定义域必须是 <font face="Times New Roman"><b><i> x </i></b></font> 所有可能状态的集合。

- 对于<font face="Times New Roman"><b><i> 	&forall; x &isin; x ,  P(x) &le; 1: </i></b></font> 不可能发生的事件概率为0，并且不存在比这概率更低的状态。类似的，能够确保一定发生的事件概率为1，而且不存在比这概率更高的状态。

- <b>归一化（normalized）</b> :  
  <div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E5%BD%92%E4%B8%80%E5%8C%96.png"/></div>



对于 <font face="Times New Roman"><b><i> x </i></b></font> 可以取  <font face="Times New Roman"><b><i> k </i></b></font> 个不同值的 <b>均匀分布（uniform distribution）</b> 为 

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83.png"/></div>


### 连续型变量和概率密度函数

当我们研究的对象是连续型随机变量时，我们用<b>概率密度函数（probability
density function, PDF）</b>而不是概率质量函数来描述它的概率分布。如果一个函数 <font face="Times New Roman"><b><i> p </i></b></font> 是概率密度函数，必须满足下面这几个条件：

<img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6.png"/>

概率密度函数 <font face="Times New Roman"><b><i> p(x) </i></b></font> 并没有直接对特定的状态给出概率，相对的，它给出了落在面积为 <font face="Times New Roman"><b><i> 	&delta;x </i></b></font> 的无限小的区域内的概率为<font face="Times New Roman"><b><i> p(x)&delta;x</i></b></font> 。


我们可以对概率密度函数求积分来获得点集的真实概率质量。特别地，<font face="Times New Roman"><b><i> x </i></b></font> 落在集合 <font face="Times New Roman"><b><i> &#120138; </i></b></font> 中的概率可以通过 <font face="Times New Roman"><b><i> p(x) </i></b></font> 对这个集合求积分来得到。在单变量的例子中，<font face="Times New Roman"><b><i> x </i></b></font> 落在区间[<font face="Times New Roman"><b>a,b</b></font>] 的概率是

## 边缘概率

有时候，我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概率分布。这种定义在子集上的概率分布被称为<b>边缘概率分布（marginal probability distribution）</b>。

例如，假设有离散型随机变量 <font face="Times New Roman"><b><i> x </i></b></font> 和 <font face="Times New Roman"><b><i> y </i></b></font>，并且我们知道 <font face="Times New Roman"><b><i> P(x, y) </i></b></font>。我们可以依据下面的<b>求和法则（sum rule）</b>来计算<font face="Times New Roman"><b><i> P(x) </i></b></font>：


<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87.png"/></div>


"边缘概率" 的名称来源于手算边缘概率的计算过程。当 <font face="Times New Roman"><b><i> P(x, y) </i></b></font> 的每个值被写在由每行表示不同的x 值，每列表示不同的 <font face="Times New Roman"><b><i> y </i></b></font> 值形成的网格中时，对网格中的每行求和是很自然的事情，然后将求和的结果 <font face="Times New Roman"><b><i> P(x) </i></b></font> 写在每行右边的纸的边缘处。

对于连续型变量，我们需要用积分替代求和：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87%E7%A7%AF%E5%88%86.png"/></div>

## 条件概率

在很多情况下，我们感兴趣的是某个事件，在给定其他事件发生时出现的概率。这种概率叫做条件概率。我们将给定 <font face="Times New Roman"><b><i> X = x，Y = y </i></b></font>  发生的条件概率记为<font face="Times New Roman"><b><i> P( X = x | Y = y ) </i></b></font>。这个条件概率可以通过下面的公式计算：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87.png"/></div>

条件概率只在 <font face="Times New Roman"><b><i> P(X = x) > 0 </i></b></font> 时有定义。我们不能计算给定在永远不会发生的事件上的条件概率。

这里需要注意的是，不要把条件概率和计算当采用某个动作后会发生什么相混淆。假定某个人说德语，那么他是德国人的条件概率是非常高的，但是如果随机选择的一个人会说德语，他的国籍不会因此而改变。计算一个行动的后果被称为<b>干预查询（intervention query）</b>。干预查询属于<b>因果模型（causal modeling）</b>的范畴，我们不会在本书中讨论。


## 条件概率的链式法则

任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相
乘的形式：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99.png"/></div>

这个规则被称为概率的<b>链式法则（chain rule）</b>或者<b>乘法法则（product rule）</b>。

## 独立性和条件独立性

两个随机变量 <font face="Times New Roman"><b><i> x </i></b></font> 和<font face="Times New Roman"><b><i> y </i></b></font>，如果它们的概率分布可以表示成两个因子的乘积形式，并且一个因子只包含 <font face="Times New Roman"><b><i> x </i></b></font>  另一个因子只包含 <font face="Times New Roman"><b><i> y </i></b></font>，我们就称这两个随机变量是<b>相互独立的（independent）</b>：

<div align=center><font face="Times New Roman"><b><i> 	&forall; x &isin; x , y &isin; y ;  p ( X = x, Y =y ) = p ( X = x )p( Y=y ): </i></b></font></div>

我们可以采用一种简化形式来表示独立性和条件独立性：<font face="Times New Roman"><b> x &perp; y </b></font> 表示  <font face="Times New Roman"><b><i> x </i></b></font> 和<font face="Times New Roman"><b><i> y </i></b></font> 相互独立，<font face="Times New Roman"><b> x &perp; y | z</b></font> 表示 <font face="Times New Roman"><b><i> x </i></b></font>  和 <font face="Times New Roman"><b><i> y </i></b></font>  在给定 <font face="Times New Roman"><b><i> z </i></b></font> 时条件独立。


## 期望、方差和协方差

### 期望

函数 <font face="Times New Roman"><b><i> f(x) </i></b></font> 关于某分布 <font face="Times New Roman"><b><i> P(x) </i></b></font> 的<b>期望（expectation）</b> 或者<b>期望值（expected value）</b>是指，当 <font face="Times New Roman"><b><i> x </i></b></font> 由 <font face="Times New Roman"><b><i> P </i></b></font> 产生，<font face="Times New Roman"><b><i> f </i></b></font> 作用于 <font face="Times New Roman"><b><i> x </i></b></font> 时，<font face="Times New Roman"><b><i> f(x) </i></b></font> 的平均值。对于离散型随机变量，这可以通过求和得到：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%9C%9F%E6%9C%9B%E7%A6%BB%E6%95%A3.png"/></div>


对于连续型随机变量可以通过求积分得到：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%9C%9F%E6%9C%9B%E7%A6%BB%E6%95%A3.png"/></div>

当概率分布在上下文中指明时，我们可以只写出期望作用的随机变量的名称来进行W简化，例如<font face="Times New Roman"><b>&Eopf;<sub><i> x </i></sub> [<i> f(x) </i>]</b></font>如果期望作用的随机变量也很明确，我们可以完全不写脚标，就像<font face="Times New Roman"><b>&Eopf; [<i> f(x) </i>]</b></font>。默认地，我们假设<font face="Times New Roman"><b>&Eopf; [&middot;]</b></font>表示对方括号内的所有随机变量的值求平均。类似的，当没有歧义时，我们还可以省略方括号。

期望是线性的，例如，
<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%9C%9F%E6%9C%9B%E5%8A%A0%E6%B3%95.png"/></div>

其中 <font face="Times New Roman"><b><i> &alpha; </i></b></font> 和<font face="Times New Roman"><b><i> 	&beta; </i></b></font> 不依赖于 <font face="Times New Roman"><b><i> x </i></b></font> 。

### 方差

<b>方差（variance）</b>衡量的是当我们对 <font face="Times New Roman"><b><i> x </i></b></font> 依据它的概率分布进行采样时，随机变量 <font face="Times New Roman"><b><i> x </i></b></font> 的函数值会呈现多大的差异：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%88%BF%E5%B7%AE.png"/></div>

当方差很小时，<font face="Times New Roman"><b><i> f(x) </i></b></font> 的值形成的簇比较接近它们的期望值。方差的平方根被称为 <b>标准差（standard deviation）</b>。

### 协方差

<b>协方差（covariance）</b>在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度：

<div align=center><font face="Times New Roman"><b>Cov( <i>f(x) , g(y)</i> ) = &Eopf; [<i>(f(x)</i> - &Eopf; [<i>f(x)</i>])  (<i>g(y)</i> - &Eopf; [<i>g(y)</i>])]</b></font></div>


协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很远。如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值。如果协方差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值，反之亦然。其他的衡量指标如<b>相关系数（correlation）</b>将每个变量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响。

协方差和相关性是有联系的，但实际上是不同的概念。它们是有联系的，因为两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么它们一定是相关的。然而，独立性又是和协方差完全不同的性质。两个变量如果协方差为零，它们之间一定没有线性关系。独立性比零协方差的要求更强，因为独立性还排除了非线性的关系。两个变量相互依赖但具有零协方差是可能的。

随机向量 <font face="Times New Roman"><b> <i>x &isin; </i> &Ropf;<sup>n</sup></b></font>  的<b>协方差矩阵（covariance matrix）</b>是一个<font face="Times New Roman"><b> n &times; n</b></font> 的矩阵，并且满足 

<div align=center><font face="Times New Roman"><b>Cov( X ) <sub><i>i , j</i></sub> = Cov ( X <sub><i>i</i></sub> , X <sub><i>j</i></sub> )</b></font></div>

设 <font face="Times New Roman"><b><i>X = [ X <sub>1</sub>, X <sub>2</sub>, X <sub>3</sub>, ... ,X <sub>n</sub> ] <sup>T</sup></i></b></font> 为 <font face="Times New Roman"><b> <i>n </i> </b></font> 维随机变量，称矩阵

<div align=center><img height ='150' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/cij.png"/></div>

为 <font face="Times New Roman"><b> <i>n </i> </b></font> 维随机变量 <font face="Times New Roman"><b> <i> X  </i> </b></font> 的协方差矩阵（covariance matrix），也记为 <font face="Times New Roman"><b> <i>D( X ) </i> </b></font>，其中

<div align=center><font face="Times New Roman"><b><i>c<sub> i , j</sub></i> = Cov ( X <sub><i>i</i></sub> , X <sub><i>j</i></sub>) ,<i>i , j = 1, 2, ... ,n</i></b></font></div> 

为 <font face="Times New Roman"><b> <i> X </i> </b></font> 的分量 <font face="Times New Roman"><b> <i> X <sub>i</sub></i> </b></font>和 <font face="Times New Roman"><b> <i> X <sub>j</sub></i> </b></font>的协方差（设它们都存在）

协方差矩阵的对角元是方差：

<div align=center><font face="Times New Roman"><b>Cov ( X <sub><i>i</i></sub> , X <sub><i>j</i></sub>) = Var ( X <sub><i>i</i></sub> )</b></font></div>

## 常见概率分布

### Bernoulli分布 [伯努利分布]

<b>Bernoulli 分布（Bernoulli distribution）</b>是单个二值随机变量的分布。它由单个参数 <font face="Times New Roman"><b><i> &straightphi; &isin; [ 0 , 1 ] </i></b></font> 控制， <font face="Times New Roman"><b><i> &straightphi;  </i></b></font> 给出了随机变量等于 1 的概率。它具有如下的一些性质：

<div align=center><img height ='200' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E4%BC%AF%E5%8A%AA%E5%8A%9B.png"/></div>

### Multinoulli 分布 


"Multinoulli" 这个术语是最近被Gustavo Lacerdo 发明、被Murphy (2012) 推广的。Multinoulli 分布是多项式分布（multinomial distribution）的一个特例。多项式分布是<font face="Times New Roman"><b> {0 , ... , <i>n</i>} <sup><i>T</i></sup>  </b></font>中的向量的分布，用于表示当对 Multinoulli 分布采样n 次时k 个类中的每一个被访问的次数。很多文章使用"多项式分布" 而实际上说的是Multinoulli 分布，但是他们并没有说是对n = 1 的情况，这点需要注意。

<b>Multinoulli 分布（multinoulli distribution）</b>或者<b>范畴分布（categorical dis-tribution）</b>是指在具有 <font face="Times New Roman"><b> <i>k</i></b></font> 个不同状态的单个离散型随机变量上的分布，其中 <font face="Times New Roman"><b> <i>k</i></b></font> 是一
个有限值。

 Multinoulli 分布由向量 <font face="Times New Roman"><b> <i>p </i> &isin; [ 0 , 1 ]<sup><i>k-1</i></sup></b></font>  参数化，其中每一个分量<font face="Times New Roman"><b> <i>p <sub>i</sub> </i> </b></font> 表示第 <font face="Times New Roman"><b> <i>i </i> </b></font> 个状态的概率。最后的第 <font face="Times New Roman"><b> <i>k </i> </b></font> 个状态的概率可以通过 <font face="Times New Roman"><b> 1 - 1<sup>&#8868;</sup><i>p</i> </b></font> 给出。注意我们必须限制 <font face="Times New Roman"><b> 1 - 1<sup>&#8868;</sup><i>p</i> &le; 1</b></font> 。Multinoulli 分布经常用来表示对象分类的分布，所以我们很少假设状态 1 具有数值 1 之类的。因此，我们通常不需要去计算Multinoulli 分布的随机变量的期望和方差。

Bernoulli 分布和 Multinoulli 分布足够用来描述在它们领域内的任意分布。它们能够描述这些分布，不是因为它们特别强大，而是因为它们的领域很简单；它们可以对那些，能够将所有的状态进行枚举的离散型随机变量进行建模。当处理的是连续型随机变量时，会有不可数无限多的状态，所以任何通过少量参数描述的概率分布都必须在分布上加以严格的限制。

### 高斯分布

实数上最常用的分布就是<b>正态分布（normal distribution）</b>，也称为<b>高斯分布（Gaussian distribution）</b>：

<div align=center><img height ='80' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83.png"/></div>

<div align=center><img height ='250' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E5%9B%BE%E5%83%8F.png"/></div>

正态分布。正态分布<font face="Times New Roman"><b> <i> &Nu;</i> ( <i>x</i> ;<i> &mu; ,  &sigma; </i>) </b></font>  呈现经典的"钟形曲线"的形状，其中中心峰的<font face="Times New Roman"><b> <i>x</i></b></font>  坐标由 <font face="Times New Roman"><b> <i>&mu;</i></b></font>  给出，峰的宽度受 <font face="Times New Roman"><b> <i>&sigma;</i></b></font> 控制。在这个示例中，我们展示的是<b>标准正态分布（standard normal distribution）</b>，其中 <font face="Times New Roman"><b> <i> &mu; = 0 , &sigma; =1</i></b></font> 

正态分布由两个参数控制，<font face="Times New Roman"><b> <i> &mu; &isin; &Ropf;</i></b></font> 和 <font face="Times New Roman"><b> <i> &sigma; &isin; </i>( 0 , &infin;)</b></font>。参数 <font face="Times New Roman"><b> <i>&mu;</i></b></font> 给出了中心峰值的坐标，这也是分布的均值：<font face="Times New Roman"><b> &Eopf;[x] = &mu; </b></font> 。分布的标准差用 <font face="Times New Roman"><b> <i>&sigma;</i></b></font> 表示，方差用 <font face="Times New Roman"><b> <i>&sigma; <sup>2</sup></i></b></font> 表示。

当我们要对概率密度函数求值时，我们需要对 <font face="Times New Roman"><b> <i>&sigma;</i></b></font>  平方并且取倒数。当我们需要经常对不同参数下的概率密度函数求值时，一种更高效的参数化分布的方式是使用参数 <font face="Times New Roman"><b> <i> &beta; &isin; </i>( 0 , &infin;)</b>，来控制分布的<b>精度（precision）</b>(或方差的倒数)：

<div align=center><img height ='80' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E9%AB%98%E6%96%AF%E5%87%BD%E6%95%B0%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6.png"/></div>

采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实
数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选
择，其中有两个原因。

第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。<b>中心极限定理（central limit theorem）</b>说明很多独立随机变量的和近似服从正态分布。这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。

第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大
的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。
充分利用和证明这个想法需要更多的数学工具。

正态分布可以推广到 <font face="Times New Roman"><b>  &Ropf; <sup><i>n</i></sup></b></font> 空间，这种情况下被称为<b>多维正态分布（multivariate normal distribution）</b>。它的参数是一个正定对称矩阵  <font face="Times New Roman"><b> <i>&sum;</i></b></font> ：

<div align=center><img height ='80' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E5%A4%9A%E7%BB%B4%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.png"/></div>

参数 <font face="Times New Roman"><b> <i>&mu;</i></b></font> 仍然表示分布的均值，只不过现在是向量值。参数 <font face="Times New Roman"><b> <i>&sum;</i></b></font> 给出了分布的协方差矩阵。和单变量的情况类似，当我们希望对很多不同参数下的概率密度函数多次求值时，协方差矩阵并不是一个很高效的参数化分布的方式，因为对概率密度函数求值时需要对 <font face="Times New Roman"><b> <i>&sum;</i></b></font> 求逆。我们可以使用一个<b>精度矩阵（precision matrix）</b> <font face="Times New Roman"><b> <i>&beta;</i></b></font> 进行替代：

<div align=center><img height ='80' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E7%B2%BE%E5%BA%A6%E7%9F%A9%E9%98%B5.png"/></div>

我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是<b>各向同性
（isotropic）高斯分布</b>，它的协方差矩阵是一个标量乘以单位阵。

### 指数分布和Laplace 分布

在深度学习中，我们经常会需要一个在 <font face="Times New Roman"><b> <i>x = 0</i></b></font> 点处取得边界点(sharp point) 的分布。为了实现这一目的，我们可以使用<b>指数分布（exponential distribution）</b>：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83.png"/></div>

指数分布使用指示函数(indicator function)<font face="Times New Roman"><b> 1 <sub><i>x &ge; 0</i></sup></b></font> 来使得当x 取负值时的概率为零。

一个联系紧密的概率分布是<b>Laplace 分布（Laplace distribution）</b>，它允许我们在任意一点 <font face="Times New Roman"><b> <i>&mu;</i></b></font> 处设置概率质量的峰值

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Laplace%20%E5%88%86%E5%B8%83.png"/></div>

### Dirac 分布和经验分布

在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通
过 <b>Dirac delta 函数（Dirac delta function）</b><font face="Times New Roman"><b> <i>&delta; (x)</i></b></font> 定义概率密度函数来实现：

<div align=center><font face="Times New Roman"><b><i>p(x) = &delta; (x - &mu; )</i>
</b></font></div>

Dirac delta 函数被定义成在除了 0 以外的所有点的值都为 0，但是积分为 1。Dirac delta 函数不像普通函数一样对x 的每一个值都有一个实数值的输出，它是一种不同类型的数学对象，被称为<b>广义函数（generalized function）</b>，广义函数是依据积分性质定义的数学对象。我们可以把Dirac delta 函数想成一系列函数的极限点，这一系列函数把除 0 以外的所有点的概率密度越变越小。

通过把 <font face="Times New Roman"><b> <i> p(x) </i></b></font> 定义成 <font face="Times New Roman"><b> <i> &delta; </i></b></font> 函数左移 <font face="Times New Roman"><b> <i> - &mu; </i></b></font> 个单位，我们得到了一个在 <font face="Times New Roman"><b> <i> x = &mu; </i></b></font> 处具有无限窄也无限高的峰值的概率质量。

Dirac 分布经常作为<b>经验分布（empirical distribution）</b>的一个组成部分出现：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83.png"/></div>

经验分布将概率密度<font face="Times New Roman"><b> <i> 1/m </i></b></font>赋给 <font face="Times New Roman"><b> <i> m </i></b></font> 个点 <font face="Times New Roman"><b> <i> x <sup>(1)</sup>, ... , x <sup>(m)</sup> </i></b></font>中的每一个，这些点是给定的数据集或者采样的集合。只有在定义连续型随机变量的经验分布时，Dirac delta 函数才是必要的。对于离散型随机变量，情况更加简单：经验分布可以被定义成一个Multinoulli 分布，对于每一个可能的输入，其概率可以简单地设为在训练集上那个输入值的<b>经验频率（empirical frequency）</b>。

当我们在训练集上训练模型时，我们可以认为从这个训练集上得到的经验分布指明了我们采样来源的分布。关于经验分布另外一种重要的观点是，它是训练数
据的似然最大的那个概率密度函数。

### 分布的混合

通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组
合方法是构造<b>混合分布（mixture distribution）</b>。混合分布由一些组件(component)分布构成。每次实验，样本是由哪个组件分布产生的取决于从一个Multinoulli 分布中采样的结果：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%83.png"/></div>

这里 <font face="Times New Roman"><b> <i> P(c) </i></b></font>是对各组件的一个 Multinoulli 分布。

混合模型使我们能够一瞥以后会用到的一个非常重要的概念——<b>潜变量 （latent variable）</b>。潜变量是我们不能直接观测到的随机变量。混合模型的组件标 识变量 <font face="Times New Roman"><b> <i> c </i></b></font> 就是其中一个例子。潜变量在联合分布中可能和 <font face="Times New Roman"><b> <i> x </i></b></font> 有关，在这种情况下， <font face="Times New Roman"><b> <i> P ( x , c ) = P ( x | c )P (c) </i></b></font>。潜变量的分布  <font face="Times New Roman"><b> <i> P ( c ) </i></b></font> 以及关联潜变量和观测变量的条件分布 <font face="Times New Roman"><b> <i> P ( x | c ) </i></b></font>，共同决定了分布 <font face="Times New Roman"><b> <i> P ( x ) </i></b></font> 的形状，尽管描述 <font face="Times New Roman"><b> <i> P ( x ) </i></b></font> 时可能并不需要潜变量。

一个非常强大且常见的混合模型是<b>高斯混合模型（Gaussian Mixture Model）</b>， 它的组件 <font face="Times New Roman"><b> <i> p ( x | c = i ) </i></b></font> 是高斯分布。每个组件都有各自的参数，均值 <font face="Times New Roman"><b> <i> &mu; <sup>( i )</sup> </i></b></font> 和协方差矩阵  <font face="Times New Roman"><b> <i> &sum; <sup>( i )</sup> </i></b></font>。有一些混合可以有更多的限制。例如，协方差矩阵可以通过  <font face="Times New Roman"><b> <i> &sum; <sup>( i )</sup> = &sum; , &forall; i </i></b></font> 的 形式在组件之间共享参数。和单个高斯分布一样，高斯混合模型有时会限制每个组 件的协方差矩阵为对角的或者各向同性的 (标量乘以单位矩阵）。

除了均值和协方差以外，高斯混合模型的参数指明了给每个组件 <font face="Times New Roman"><b> <i> i </i></b></font> 的<b>先验概率 （prior probability）</b> <font face="Times New Roman"><b> <i> &alpha; <sub>i</sub> = P (c = i) </i></b></font>。"先验" 一词表明了在观测到 <font face="Times New Roman"><b> <i> x </i></b></font> 之前传递给模 型关于 <font face="Times New Roman"><b> <i> c </i></b></font> 的信念。作为对比，<font face="Times New Roman"><b> <i> P ( x | c ) </i></b></font> 是<b>后验概率（posterior probability）</b>，因为它 是在观测到 <font face="Times New Roman"><b> <i> x </i></b></font> 之后进行计算的。高斯混合模型是概率密度的万能近似器（universal approximator），在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。

下图演示了某个高斯混合模型生成的样本。

<div align=center><img height ='250' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B.png"/></div>

来自高斯混合模型的样本。在这个示例中，有三个组件。从左到右，第一个组件具有各向 同性的协方差矩阵，这意味着它在每个方向上具有相同的方差。第二个组件具有对角的协方差矩阵，这意味着它可以沿着每个轴的对齐方向单独控制方差。该示例中，沿着 <font face="Times New Roman"><b> <i> x <sub>1</sub> </i></b></font> 轴的方差要比沿着 <font face="Times New Roman"><b> <i> x <sub>2</sub> </i></b></font> 轴的方差大。第三个组件具有满秩的协方差矩阵，使它能够沿着任意基的方向单独地控制方差。



## 常用函数的有用性质

某些函数在处理概率分布时经常会出现，尤其是深度学习的模型中用到的概率分布。

其中一个函数是 <b>logistic sigmoid 函数</b>

<div align=center><img height ='70' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/logistic%20sigmoid.png"/></div>

logistic sigmoid 函数通常用来产生 Bernoulli 分布中的参数 <font face="Times New Roman"><b> <i> &phi; </i></b></font>，因为它的范围是 ( 0 , 1 )，处在 <font face="Times New Roman"><b> <i> &phi; </i></b></font> 的有效取值范围内。下图给出了 sigmoid 函数的图示。sigmoid 函数 在变量取绝对值非常大的正值或负值时会出现饱和（saturate）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感

<div align=center><img height ='250' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/logistic%20sigmoid%20%E5%9B%BE.png"/></div>

另外一个经常遇到的函数是 <b>softplus 函数（softplus function）</b>(Dugas et al.,2001)：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/softplus%E5%87%BD%E6%95%B0.png"/></div>

softplus 函数可以用来产生正态分布的 <font face="Times New Roman"><b> <i> &beta; </i></b></font> 和 <font face="Times New Roman"><b> <i> &sigma; </i></b></font> 参数，因为它的范围是 ( 0 , &infin; )。当处 理包含 sigmoid 函数的表达式时它也经常出现。softplus 函数名来源于它是另外一个函数的平滑（或 "软化"）形式，这个函数是

<div align=center><font face="Times New Roman"><b><i> x <sup>+</sup> = max ( 0 , x ) </i></b></font></div>

下面一些性质非常有用：

<div align=center><img height ='400' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E5%A4%9A%E4%B8%AA%E6%80%A7%E8%B4%A8.png"/></div>

函数 <font face="Times New Roman"><b> <i> &sigma; <sup>-1</sup> </i></b></font> 在统计学中被称为 <b>分对数（logit）</b>，但这个函数在机器学习中很少用到。


<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/softplus%E5%87%BD%E6%95%B0%E7%89%B9%E5%BE%81.png"/></div>

上式为函数名 “softplus’’ 提供了其他的正当理由。softplus 函数被设计成正 部函数（positive part function）的平滑版本，这个正部函数是指 <font face="Times New Roman"><b><i> x <sup>+</sup> = max ( 0 , x ) </i></b></font>。 与正部函数相对的是<b>负部函数（negative part function）</b> <font face="Times New Roman"><b><i> x <sup>-</sup > = max ( 0 , - x ) </i></b></font>。为了获得类似负部函数的一个平滑函数，我们可以使用 <font face="Times New Roman"><b> <i> &zeta; ( - x ) </i></b></font>。就像 <font face="Times New Roman"><b> <i> x </i></b></font> 可以用它的正部和负部通过等式 <font face="Times New Roman"><b> <i> x <sup>+</sup> - x <sup>-</sup> = x</i></b></font> 恢复一样，我们也可以用同样的方式对 <font face="Times New Roman"><b> <i> &zeta; (  x ) </i></b></font> 和 <font face="Times New Roman"><b> <i> &zeta; ( - x ) </i></b></font> 进行操作，就像上式中那样。

## 贝叶斯规则

我们经常会需要在已知 <font face="Times New Roman"><b> <i> P ( y | x ) </i></b></font> 时计算 <font face="Times New Roman"><b> <i> P ( x | y ) </i></b></font>。幸运的是，如果还知道 <font face="Times New Roman"><b> <i> P ( x ) </i></b></font>， 我们可以用<b>贝叶斯规则（Bayes’ rule）</b>来实现这一目的：

<div align=center><img height ='70' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E8%B4%9D%E5%8F%B6%E6%96%AF.png"/></div>

注意到 <font face="Times New Roman"><b> <i> P ( y ) </i></b></font> 出现在上面的公式中，它通常使用 <font face="Times New Roman"><b> <i>P ( y ) = &sum; <sub>x</sub> P ( y | x) P ( y ) </i></b></font> 来计算，所以我们并不需要事先知道 <font face="Times New Roman"><b> <i> P ( y ) </i></b></font> 的信息。

## 连续型变量的技术细节

连续型随机变量和概率密度函数的深入理解需要用到数学分支<b>测度论（measure theory）</b>的相关内容来扩展概率论。测度论超出了本书的范畴，但我们可以简要勾勒 一些测度论用来解决的问题。

我们已经看到连续型向量值随机变量 <font face="Times New Roman"><b> <i> x </i></b></font> 落在某个集合 <font face="Times New Roman"><b> <i> &#120138; </i></b></font> 中的概率是通过 <font face="Times New Roman"><b> <i> p(x) </i></b></font> 对集合 <font face="Times New Roman"><b> <i> &#120138; </i></b></font>  积分得到的。对于集合 <font face="Times New Roman"><b> <i> &#120138; </i></b></font>  的一些选择可能会引起悖论。例 如，构造两个集合 <font face="Times New Roman"><b> <i> &#120138; <sub>1</sub></i></b></font>  和 <font face="Times New Roman"><b> <i> &#120138; <sub>2</sub></i></b></font> 使得 <font face="Times New Roman"><b> <i> p ( x &isin; &#120138; <sub>1</sub> ) + p ( x &isin; S <sub>1</sub> ) > 1 </i></b></font> 并且 <font face="Times New Roman"><b> <i> &#120138; <sub>1</sub> &cap; &#120138; <sub>2</sub> = ∅</i></b></font>   是可能 的。这些集合通常是大量使用了实数的无限精度来构造的，例如通过构造分形形状 (fractal-shaped) 的集合或者是通过有理数相关集合的变换定义的集合。测度论的 一个重要贡献就是提供了一些集合的特征使得我们在计算概率时不会遇到悖论。在 本书中，我们只对相对简单的集合进行积分，所以测度论的这个方面不会成为一个 相关考虑。

对于我们的目的，测度论更多的是用来描述那些适用于 <font face="Times New Roman"><b>  &Ropf; <sup>n</sup> </b></font> 上的大多数点，却不 适用于一些边界情况的定理。测度论提供了一种严格的方式来描述那些非常微小的 点集。这种集合被称为 <b>"零测度（measure zero）"</b> 的。我们不会在本书中给出这个 概念的正式定义。然而，直观地理解这个概念是有用的，我们可以认为零测度集在 我们的度量空间中不占有任何的体积。例如，在 <font face="Times New Roman"><b>  &Ropf; <sup>2</sup> </b></font>  空间中，一条直线的测度为零， 而填充的多边形具有正的测度。类似的，一个单独的点的测度为零。可数多个零测 度集的并仍然是零测度的 (所以所有有理数构成的集合测度为零)。
 
另外一个有用的测度论中的术语是 <b>"几乎处处（almost everywhere）"</b>。某个性 质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都是成立的。因为这些例外只在空间中占有极其微小的量，它们在多数应用中都可以 被放心地忽略。概率论中的一些重要结果对于离散值成立但对于连续值只能是 "几乎处处" 成立。

连续型随机变量的另一技术细节，涉及到处理那种相互之间有确定性函数关系 的连续型变量。假设我们有两个随机变量 <font face="Times New Roman"><b><i> x </i></b></font> 和 <font face="Times New Roman"><b><i> y </i></b></font> 满足 <font face="Times New Roman"><b><i>  y = g ( x ) </i></b></font>，其中 <font face="Times New Roman"><b><i> g </i></b></font> 是可逆的、 连续可微的函数。可能有人会想 <font face="Times New Roman"><b><i>  p <sub>y</sub> ( y )= p <sub>x</sub> ( g <sup>-1</sup> ( y ) ) </i></b></font>。但实际上这并不对。

举一个简单的例子，假设我们有两个标量值随机变量 <font face="Times New Roman"><b><i> x </i></b></font> 和 <font face="Times New Roman"><b><i> y </i></b></font>，并且满足 <font face="Times New Roman"><b><i> y = x / 2 </i></b></font> 以及 <font face="Times New Roman"><b><i> x </i> ~ U ( 0 ,1 ) </b></font>。如果我们使用 <font face="Times New Roman"><b><i>  p <sub>y</sub> ( y )= p <sub>x</sub> ( 2 y  ) </i></b></font>，那么 font face="Times New Roman"><b><i>  p <sub>y</sub>  </i></b></font> 除了区间 [ 0 , 1 / 2 ] 以外都为 0，并且在这个区间上的值为 1。这意味着

<div align=center><img height ='60' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E8%BF%9E%E7%BB%AD%E5%87%BD%E6%95%B0.png"/></div>

而这违背了概率密度的定义 (积分为 1)。这个常见错误之所以错是因为它没有考虑到引入函数 <font face="Times New Roman"><b><i> g </i></b></font> 后造成的空间变形。回忆一下，<font face="Times New Roman"><b><i> x </i></b></font> 落在无穷小的体积为 <font face="Times New Roman"><b><i> &delta;x </i></b></font> 的区域内的 概率为  <font face="Times New Roman"><b><i> p(x)&delta;x </i></b></font>。因为 <font face="Times New Roman"><b><i> g </i></b></font> 可能会扩展或者压缩空间，在 <font face="Times New Roman"><b><i> x </i></b></font> 空间内的包围着 <font face="Times New Roman"><b><i> x </i></b></font> 的无穷小体积在 <font face="Times New Roman"><b><i> y </i></b></font> 空间中可能有不同的体积

为了看出如何改正这个问题，我们回到标量值的情况。我们需要保持下面这个性质：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/1.png"/></div>

求解上式，我们得到

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/2.png"/></div>

或者等价地，

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/3.png"/></div>

在高维空间中，微分运算扩展为<b>Jacobian 矩阵（Jacobian matrix）</b>的行列式—— 矩阵的每个元素如下。

<div align=center><img height ='40' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/4.png"/></div>


因此，对于实值向量 <font face="Times New Roman"><b><i> x </i></b></font> 和 <font face="Times New Roman"><b><i> y </i></b></font>，

<div align=center><img height ='60' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/5.png"/></div>

# 信息论

信息论(information theory)是将信息的传递作为一种统计现象来研究，如通过某种编码方式要传递某种信息所需要的信道带宽或比特数。我们想要定量的描述随机事件的信息量需要满足如下性质：

- 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件 应该没有信息量。极端情况下，如果一个事件100%确定发生，那么它所包含的信息量为零。
- 较不可能发生的事件具有更高的信息量。
- 独立事件应具有增量的信息即独立事件的信息量可叠加。。例如，投掷的硬币两次正面朝上传递的信息量， 应该是投掷一次硬币正面朝上的信息量的两倍。

为了满足上述三个性质，我们定义一个事件 <font face="Times New Roman"><b><i> X = x </i></b></font> 的<b>自信息（self-information）</b> 为

<div align=center><font face="Times New Roman"><b><i> I = - ln P (x) </i></b></font></div>

我们定义的 <font face="Times New Roman"><b><i> I (x) </i></b></font> 单位是<b>奈特（nats）</b>。一奈特是以单位是 <font face="Times New Roman"><b><i> 1 / e </i></b></font>的概率观测到一个事件时获得的信息量。其他的材料中使用底数为 2 的对数，单位是<b>比特（bit）</b>或者<b>香农（shannons）</b>；通过比特度 量的信息只是通过奈特度量信息的常数倍。

当 <font face="Times New Roman"><b><i> x </i></b></font> 是连续的，我们使用类似的关于信息的定义，但有些来源于离散形式的性 质就丢失了。例如，一个具有单位密度的事件信息量仍然为 0，但是不能保证它一定 发生。

自信息只处理单个的输出。我们可以用<b>香农熵（Shannon entropy）</b>来对整个概率分布中的不确定性总量进行量化：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E9%A6%99%E5%86%9C%E7%86%B52.png"/></div>

即

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E9%A6%99%E5%86%9C%E7%86%B5.png"/></div>

也记作 <font face="Times New Roman"><b><i> H(P) </i></b></font>。换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。它给出了对依据概率分布 <font face="Times New Roman"><b><i> P </i></b></font> 生成的符号进行编码所需的比特数在平均意义上的下界 (当对数底数不是 2 时，单位将有所不同)。那些接近确定性的分布 (输出几 乎可以确定) 具有较低的熵；那些接近均匀分布的概率分布具有较高的熵。下图给出了一个说明。当 <font face="Times New Roman"><b><i> x </i></b></font> 是连续的，香农熵被称为<b>微分熵（diﬀerential entropy）</b>。

<div align=center><img height ='250' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E5%BE%AE%E5%88%86%E7%86%B5.png"/></div>


二值随机变量的香农熵。该图说明了更接近确定性的分布是如何具有较低的香农熵，而更接近均匀分布的分布是如何具有较高的香农熵。水平轴是  <font face="Times New Roman"><b><i> p </i></b></font>，表示二值随机变量等于 1 的概率。熵由 <font face="Times New Roman"><b><i> ( p - 1 ) ln ( 1 - p ) - p ln p </i></b></font> 给出。当 <font face="Times New Roman"><b><i> p </i></b></font> 接近 0 时，分布几乎是确定的，因为随机变量几乎总是 0。当 <font face="Times New Roman"><b><i> p </i></b></font> 接近 1 时，分布也几乎是确定的，因为随机变量几乎总是 1。当 <font face="Times New Roman"><b><i> p = 0.5 </i></b></font> 时，熵是最大的， 因为分布在两个结果（0 和 1）上是均匀的。


香农熵越大，则描述该系统所需的比特数越大，而对于确定性的非随机的系统，其香农熵很小。

如果我们对于同一个随机变量 <font face="Times New Roman"><b><i> x </i></b></font> 有两个单独的概率分布 <font face="Times New Roman"><b><i> P ( x ) </i></b></font> 和 <font face="Times New Roman"><b><i> Q ( x ) </i></b></font>，我们可 以使用<b>KL散度（Kullback-Leibler (KL) divergence）</b>来衡量这两个分布的差异

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/KL%E6%95%A3%E5%BA%A6.png"/></div>

在离散型变量的情况下，KL 散度衡量的是，当我们使用一种被设计成能够使 得概率分布 <font face="Times New Roman"><b><i> Q </i></b></font> 产生的消息的长度最小的编码，发送包含由概率分布 <font face="Times New Roman"><b><i> P </i></b></font> 产生的符号的消息时，所需要的额外信息量 (如果我们使用底数为 2 的对数时，信息量用比特衡量，但在机器学习中，我们通常用奈特和自然对数。)

它表示了假如我们采取某种编码方式使编码Q分布所需的比特数最少，那么编码P分布所需的额外的比特数。假如P和Q分布完全相同，则其KL divergence 为零。

KL 散度有很多有用的性质，最重要的是它是非负的。KL 散度为 0 当且仅当 <font face="Times New Roman"><b><i> P </i></b></font> 和 <font face="Times New Roman"><b><i> Q </i></b></font> 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 "几乎处处" 相同的。因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常 被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的：对于某 些 <font face="Times New Roman"><b><i> P </i></b></font> 和 <font face="Times New Roman"><b><i> Q </i></b></font>，<font face="Times New Roman"><b><i> D <sub>KL</sub>(  P || Q ) &NotEqual; D <sub>KL</sub>( Q || P )</i></b></font>。这种非对称性意味着选择 <font face="Times New Roman"><b><i> D <sub>KL</sub>(  P || Q ) </i></b></font> 还是<font face="Times New Roman"><b><i> D <sub>KL</sub>( Q || P )</i></b></font> 影响很大

<div align=center><img height ='250' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/KL.png"/></div>

 KL 散度是不对称的。假设我们有一个分布 <font face="Times New Roman"><b><i> p ( x ) </i></b></font> ，并且希望用另一个分布 <font face="Times New Roman"><b><i> q ( x ) </i></b></font>来近似它。 我们可以选择最小化 <font face="Times New Roman"><b><i> D <sub>KL</sub>(  p || q ) </i></b></font> 或最小化 <font face="Times New Roman"><b><i> D <sub>KL</sub>(  q || p ) </i></b></font>。为了说明每种选择的效果，我们令  <font face="Times New Roman"><b><i> p  </i></b></font> 是两 个高斯分布的混合，令 <font face="Times New Roman"><b><i> q </i></b></font> 为单个高斯分布。选择使用 KL 散度的哪个方向是取决于问题的。一些应用需要这个近似分布 <font face="Times New Roman"><b><i> q  </i></b></font>  在真实分布 <font face="Times New Roman"><b><i> p  </i></b></font>  放置高概率的所有地方都放置高概率，而其他应用需要这个近似分布 <font face="Times New Roman"><b><i> q  </i></b></font> 在真实分布 <font face="Times New Roman"><b><i> p  </i></b></font>放置低概率的所有地方都很少放置高概率。KL 散度方向的选择反映 了对于每种应用，优先考虑哪一种选择。(左) 最小化 <font face="Times New Roman"><b><i> D <sub>KL</sub>(  p || q ) </i></b></font> 的效果。在这种情况下，我们选 择一个 <font face="Times New Roman"><b><i> q  </i></b></font>  使得它在 <font face="Times New Roman"><b><i> p  </i></b></font>  具有高概率的地方具有高概率。当 <font face="Times New Roman"><b><i> p  </i></b></font>  具有多个峰时，<font face="Times New Roman"><b><i> q  </i></b></font>  选择将这些峰模糊到一起，以便将高概率质量放到所有峰上。(右) 最小化 <font face="Times New Roman"><b><i> D <sub>KL</sub>(  q || p ) </i></b></font> 的效果。在这种情况下，我们选择一个 <font face="Times New Roman"><b><i> q  </i></b></font>  使得它在 <font face="Times New Roman"><b><i> p  </i></b></font>  具有低概率的地方具有低概率。当 <font face="Times New Roman"><b><i> p  </i></b></font>  具有多个峰并且这些峰间隔很宽时，如 该图所示，最小化 KL 散度会选择单个峰，以避免将概率质量放置在 <font face="Times New Roman"><b><i> p  </i></b></font>  的多个峰之间的低概率区域中。这里，我们说明当 <font face="Times New Roman"><b><i> q  </i></b></font>  被选择成强调左边峰时的结果。我们也可以通过选择右边峰来得到 KL 散度相同的值。如果这些峰没有被足够强的低概率区域分离，那么 KL 散度的这个方向仍然可能 选择模糊这些峰。

 一个和 KL 散度密切联系的量是<b>交叉熵（cross-entropy）</b> <font face="Times New Roman"><b><i> I = - ln P (x) </i></b></font>，它和 KL 散度很像但是缺少左边一项

 <div align=center><font face="Times New Roman"><b><i> H ( P , Q ) = - &Eopf; <sub>X ~ P</sub> ln Q(x) </i></b></font></div>

针对 <font face="Times New Roman"><b><i> Q </i></b></font> 最小化交叉熵等价于最小化 KL 散度，因为 <font face="Times New Roman"><b><i> Q </i></b></font> 并不参与被省略的那一项。

当我们计算这些量时，经常会遇到 0ln0 这个表达式。按照惯例，在信息论中， 我们将这个表达式处理为 <font face="Times New Roman"><b><i> lim <sub>x 	&rarr; 0</sub> x ln x =0</i></b></font>。

# 结构化概率模型

机器学习的算法经常会涉及到在非常多的随机变量上的概率分布。通常，这些概率分布涉及到的直接相互作用都是介于非常少的变量之间的。使用单个函数来描述整个联合概率分布是非常低效的 (无论是计算上还是统计上)。

我们可以把概率分布分解成许多因子的乘积形式，而不是使用单一的函数来表 示概率分布。例如，假设我们有三个随机变量 <font face="Times New Roman"><b><i> a , b </i></b></font> 和 font face="Times New Roman"><b><i> c </i></b></font>，并且 <font face="Times New Roman"><b><i> a  </i></b></font> 影响 <font face="Times New Roman"><b><i>  b </i></b></font> 的取值，<font face="Times New Roman"><b><i>  b </i></b></font> 影 响 <font face="Times New Roman"><b><i>  c </i></b></font> 的取值，但是 <font face="Times New Roman"><b><i>  a </i></b></font> 和 <font face="Times New Roman"><b><i>  c </i></b></font> 在给定 <font face="Times New Roman"><b><i>  b </i></b></font> 时是条件独立的。我们可以把全部三个变量的概率分布重新表示为两个变量的概率分布的连乘形式：

<div align=center><font face="Times New Roman"><b><i> p ( a , b , c ) = p ( a ) p (b | a) p ( c | b ) </i></b></font></div>

这种分解可以极大地减少用来描述一个分布的参数数量。每个因子使用的参数 数目是它的变量数目的指数倍。这意味着，如果我们能够找到一种使每个因子分布具有更少变量的分解方法，我们就能极大地降低表示联合分布的成本。

我们可以用图来描述这种分解。这里我们使用的是图论中的 "图" 的概念：由 一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分解，我们把它称为<b>结构化概率模型（structured probabilistic model）</b>或者<b>图模型 （graphical model）</b>。

有两种主要的结构化概率模型：有向的和无向的。两种图模型都使用图 	<font face="Times New Roman"><b><i> &Gcedil; </i></b></font>，其中 图的每个节点对应着一个随机变量，连接两个随机变量的边意味着概率分布可以表 示成这两个随机变量之间的直接作用。

<b>有向（directed）</b>模型使用带有有向边的图，它们用条件概率分布来表示分解， 就像上面的例子。特别地，有向模型对于分布中的每一个随机变量 <font face="Times New Roman"><b><i> x <sub>i</sub> </i></b></font> 都包含着一个 影响因子，这个组成  <font face="Times New Roman"><b><i> x <sub>i</sub> </i></b></font> 条件概率的影响因子被称为  <font face="Times New Roman"><b><i> x <sub>i</sub> </i></b></font> 的父节点，记为  <font face="Times New Roman"><b><i> Pa<sub> &Gcedil;</sub> ( x <sub>i</sub> ) </i></b></font>：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%9C%89%E5%90%91.png"/></div>

下给出了一个有向图的例子以及它表示的概率分布的分解。

<div align=center><img height ='250' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%9C%89%E5%90%912.png"/></div>

于随机变量<font face="Times New Roman"><b><i> a , b , c, d</sub> </i></b></font> 和 <font face="Times New Roman"><b><i> e</sub> </i></b></font> 的有向图模型。这幅图对应的概率分布可以分解为

<div align=center><font face="Times New Roman"><b><i> p ( a , b , c , d , e  ) = p ( a ) p (b | a) p ( c | a , b ) p ( d | b) p (e | c)</i></b></font></div>

该图模型使我们能够快速看出此分布的一些性质。例如，<font face="Times New Roman"><b><i> a </sub> </i></b></font> 和 <font face="Times New Roman"><b><i> c </sub> </i></b></font> 直接相互影响，但 <font face="Times New Roman"><b><i> a </sub> </i></b></font> 和 <font face="Times New Roman"><b><i> e </sub> </i></b></font> 只有通过 <font face="Times New Roman"><b><i> c </sub> </i></b></font> 间接相互影响。

<b>无向（undirected）</b>模型使用带有无向边的图，它们将分解表示成一组函数；不 像有向模型那样，这些函数通常不是任何类型的概率分布。<font face="Times New Roman"><b><i> &Gcedil; </i></b></font> 中任何满足两两之间有边连接的顶点的集合被称为团。无向模型中的每个团 <font face="Times New Roman"><b><i> &#1017; <sup>( i )</sup> </i></b></font> 都伴随着一个因子 <font face="Times New Roman"><b><i>&Phi;<sup>( i )</sup>( &#1017; <sup>( i )</sup>) </i></b></font>。这些因子仅仅是函数，并不是概率分布。每个因子的输出都必须是非负的，但是并没有像概率分布中那样要求因子的和或者积分为 1

随机变量的联合概率与所有这些因子的乘积<b>成比例（proportional）</b>——意味着 因子的值越大则可能性越大。当然，不能保证这种乘积的求和为 1。所以我们需要除 以一个归一化常数 <font face="Times New Roman"><b><i> Z </i></b></font> 来得到归一化的概率分布，归一化常数 <font face="Times New Roman"><b><i> Z </i></b></font> 被定义为 <font face="Times New Roman"><b><i>&Phi; </i></b></font> 函数乘 积的所有状态的求和或积分。概率分布为：

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%97%A0%E5%90%91.png"/></div>

下图给出了一个无向图的例子以及它表示的概率分布的分解。

<div align=center><img height ='250' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%97%A0%E5%90%912.png"/></div>

关于随机变量 <font face="Times New Roman"><b><i> a , b , c, d</sub> </i></b></font> 和 <font face="Times New Roman"><b><i> e </sub> </i></b></font> 的无向图模型。这幅图对应的概率分布可以分解为

<div align=center><img height ='50' src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/%E6%97%A0%E5%90%913.png"/></div>

该图模型使我们能够快速看出此分布的一些性质。例如，<font face="Times New Roman"><b><i> a </sub> </i></b></font> 和 <font face="Times New Roman"><b><i> c </sub> </i></b></font> 直接相互影响，但 <font face="Times New Roman"><b><i> a </sub> </i></b></font> 和 <font face="Times New Roman"><b><i> e </sub> </i></b></font> 只有通过 <font face="Times New Roman"><b><i> c </sub> </i></b></font> 间接相互影响。

请记住，这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互相排斥的概率分布族。有向或者无向不是概率分布的特性；它是概率分布的一种<b>特殊描述（description）</b>所具有的特性，而任何概率分布都可以用这两种方式进行描 述。

<!----&Eopf; &Ropf; &sigma; &isin;------>