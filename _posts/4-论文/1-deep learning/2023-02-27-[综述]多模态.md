---
title: 【综述】多模态(视觉上的任务，视觉问答，图文检索，VQA，视觉推理，视觉蕴含【传统多模态任务】)
date: 2023-02-27 07:00:00 +/-0800
categories: [论文,深度学习]
tags: [深度学习,论文]     # TAG names should always be lowercase 标记名称应始终为小写
math: true
mermaid: true
---

【遗留的问题：I3D对Transformer有扩展吗】

相关文章：
1. <https://zhuanlan.zhihu.com/p/471570185>

## 之前的工作

目标检测从视觉端拿掉，取消掉预训练的视觉特征提取(21 visual Transformer解决了这个问题)

### Vilt

基于pathc视觉特征和目标检测中基于boundbox的视觉特征没有什么区别，降低了文本特征和视觉特征提取的计算量

【视觉端的模型要比文本端的模型要大，推理：复杂的数据类型需要更大特征提取，也会带来更大的计算量】

Vilt的不足：

1. 文本端的特征提取用了预训练但是视觉端没有使用
2. 推理快速，但是训练很慢

【结构上简化了，但是实践中很难节省事件】

### CLIP

VE=TE>MI【典型双塔，训练对比学习】

总结

1. 图像匹配、图像检索性能好，高效。【CLIP可以提前抽好特征】
2. VQA、VR、VE任务上性能不够，简单的点乘无法实现很好的特征融合

### 基于上述工作的结论

使用一个更大的视觉模型和更好的模态融合的模型对多模态有效果，最优修的结构应该为VE>MI>TE

视觉模型从目标检测换成visualTransform

LOSS:
（1）图像文本对比学习（ITC）（2）图像文本匹配（ITM）（3）掩码语言建模（MLM）

1. ITC【图像和文本的特征应该比较接近 Image-Text Contrastive Learning】 
2. WPA【目标检测，c类任务常用，现在放弃，文本、patch配对】 
3. MLM【Mask Language Modeling 遮住文本预测任务】 ITM【Image Text Matching Loss】
4. ITM【Image Text Matching】【给定图片和文本通过特征提取进行二分类，判断他们是否是一对】

### Align before Fuse: Vision and Language Representation Learning with Momentum Distillation 融合之前的对齐：利用动量蒸馏的视觉和语言表征学习(ALBF)

Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, Steven Hoi

Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multi-modal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR2, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at this https URL.

大规模视觉和语言表示学习已经显示出对各种视觉语言任务的有希望的改进。 大多数现有方法采用基于变换器的多模式编码器来联合建模视觉标记（基于区域的图像特征）和单词标记。 由于视觉标记和单词标记未对齐，因此多模态编码器学习图像-文本交互具有挑战性。 在本文中，我们引入了一种对比损失，通过跨模态注意力在融合图像和文本表示之前对齐它们 (ALBEF)，从而实现更扎实的视觉和语言表示学习。 与大多数现有方法不同，我们的方法不需要边界框注释或高分辨率图像。 为了改进从嘈杂的网络数据中学习，我们提出了动量蒸馏，这是一种从动量模型产生的伪目标中学习的自训练方法。 【使用MOCO产生伪标签进行学习】我们从互信息最大化的角度对 ALBEF 进行了理论分析，表明不同的训练任务可以解释为为图像-文本对生成视图的不同方式。 ALBEF 在多个下游视觉语言任务上实现了最先进的性能。 在图像文本检索中，ALBEF 优于在更大数量级的数据集上预训练的方法。 在 VQA 和 NLVR2 上，ALBEF 相对于 state-of-the-art 实现了 2.37% 和 3.84% 的绝对提升，同时享受更快的推理速度。 此 https URL 提供代码和预训练模型。

>满足之前的结论
>视觉特征和文本特征不一致，视觉编码器没有训练
>
>1. align before fuse
>2. 克服了web noisy data【文本没有很好描述图片的数据】
>3. 图片视觉提取器：DEiT,文本预训练：Bert
>4. 8卡机3、4天
>5. Momentum Model
>
>ITC:正样本和负样本对配对，很多负样本由momentum model提供【有待进一步研究】
>ITM:把图像文本融合后的特征通过FC层进行二分类，判断是否是同一个类（这个任务太简单，判断不是很容易，提高其有效性通过更困难的负样本实现，ALBF选择ITC中除自己以外相似度最高的样本作为负样本对）
>MLM(却是的)
>ALBF训练一次，需要两次前向过程。因为损失函数ITM和MLM使用的输入不一致。ITM使用了原始的图片和文本但是MLM使用了原始的图片和遮住的文本【很多任务有多次前向过程，导致很多多模态训练比较耗时】
加大难度，选取最接近的负样本
>
>动量蒸馏，web noisy data会导致损失函数有偏差。即基准的描述会出现问题。
>自监督：noisy student、DINO
>很多模型提供了exponential-moving-average（EMA）选项
>训练出模型的预测和基准比较接近，也和Moments训练出的结果接近
>MLM ITC有动量的损失函数

### VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Furu Wei【多模态领域成果颇丰】

We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA, NLVR2 and image-text retrieval. The code and pretrained models are available at this https URL.

我们提出了一个统一的视觉语言预训练模型 (VLMo)，它通过模块化 Transformer 网络共同学习双编码器和融合编码器。 具体来说，我们引入了混合模态专家 (MoME) Transformer，其中每个块都包含一个模态特定专家池和一个共享的自注意力层。 由于 MoME 的建模灵活性，预训练的 VLMo 可以作为视觉语言分类任务的融合编码器进行微调，或用作双编码器以进行高效的图像文本检索。 此外，我们提出了一种阶段性的预训练策略，它可以有效地利用除图像-文本对之外的大规模纯图像和纯文本数据。 实验结果表明，VLMo 在各种视觉语言任务上取得了最先进的结果，包括 VQA、NLVR2 和图像文本检索。 代码和预训练模型可在此 https URL 上获得。

>模型改进
>训练改进，多阶段预训练
>
>之前的模型都是双塔模型，加上多层的融合

Vision-Language (VL) pre-training [30, 41, 35, 26, 20, 23] learns generic cross-modal representations from large-scale image-text pairs. Previous models usually employ image-text matching, image-text contrastive learning, masked region classification/feature regression, word-region/patch alignment and masked language modeling to aggregate and align visual and linguistic information. Then the pretrained models can be directly fine-tuned on downstream vision-language tasks, such as VL retrieval and classification (visual question answering, visual reasoning, etc.).

视觉语言 (VL) 预训练 [30、41、35、26、20、23] 从大规模图像文本对中学习通用的跨模态表示。 以前的模型通常采用图像文本匹配、图像文本对比学习、掩蔽区域分类/特征回归、词区域/补丁对齐和掩蔽语言建模来聚合和对齐视觉和语言信息。 然后，预训练模型可以直接在下游视觉语言任务上进行微调，例如 VL 检索和分类（视觉问答、视觉推理等）。

Two mainstream architectures are widely used in previous work. CLIP [35] and ALIGN [18] adopt a dual-encoder architecture to encode images and text separately. Modality interaction is handled by the cosine similarity of the image and text feature vectors. The dual-encoder architecture is effective for retrieval tasks, especially for masses of images and text. Feature vectors of images and text can be pre-computed and stored. However, the shallow interaction between images and text is not enough to handle complex VL classification tasks. ViLT [20] finds that CLIP gives a relatively low accuracy on visual reasoning task. Another line of work [30, 41, 43, 3, 20, 23] relies on a fusion encoder with cross-modal attention to model image-text pairs. Multi-layer Transformer [45] networks are usually employed to fuse image and text representations. The fusion-encoder architecture achieves superior performance on VL classification tasks. But it requires to jointly encode all possible image-text pairs to compute similarity scores for retrieval tasks. The quadratic time complexity leads to a much slower inference speed than the dual-encoder models whose time complexity is linear.

在以前的工作中，有两种主流架构被广泛使用。CLIP[35]和ALIGN[18]采用双编码器架构对图像和文本分别进行编码。模态互动由图像和文本特征向量的余弦相似度来处理。双编码器结构对检索任务是有效的，特别是对大量的图像和文本。图像和文本的特征向量可以被预先计算和存储。然而，图像和文本之间的浅层互动不足以处理复杂的VL分类任务。ViLT[20]发现CLIP在视觉推理任务上给出了一个相对较低的准确性。另一条工作路线[30, 41, 43, 3, 20, 23]依靠一个具有跨模式关注的融合编码器来对图像-文本对进行建模。多层变换器[45]网络通常被用来融合图像和文本表示。融合编码器架构在VL分类任务上取得了卓越的性能。但它需要对所有可能的图像-文本对进行联合编码，以计算检索任务的相似度分数。二次方时间复杂度导致推理速度比时间复杂度为线性的双编码器模型慢得多。

In order to take advantage of the two types of architectures, we propose a unified Vision-Language pretrained Model (VLMO) that can be used as either a dual encoder to separately encode images and text for retrieval tasks, or used as a fusion encoder to model the deep interaction of imagetext pairs for classification tasks. This is achieved by introducing Mixture-of-Modality-Experts(MOME) Transformer that can encode various modalities (images, text, and image-text pairs) within a Transformer block. MOME employs a pool of modality experts to replace the feed-forward network in standard Transformer. It captures modality-specific information by switching to different modality experts, and uses the shared self-attention across modalities to align visual and linguistic information. Specifically, MOME Transformer consists of three modality experts, namely vision expert for image encoding, language expert for text encoding, and vision-language expert for image-text fusion. Thanks to the modeling flexibility, we can reuse MOME Transformer with the shared parameters for different purposes, i.e., text-only encoder, image-only encoder, and image-text fusion encoder.

为了利用这两类架构的优势，我们提出了一个统一的视觉-语言预训练模型（VLMO），它既可以作为一个双编码器，为检索任务单独编码图像和文本，也可以作为一个融合编码器，为分类任务建立图像-文本对的深度互动模型。这是通过引入混合模式专家（MOME）变换器来实现的，该变换器可以在一个变换器块中对各种模式（图像、文本和图像-文本对）进行编码。MOME采用了一个模态专家库来取代标准Transformer中的前馈网络。它通过切换到不同的模态专家来捕捉特定的模态信息，并利用跨模态的共享自我注意力来调整视觉和语言信息。具体来说，MOME转化器由三个模态专家组成，即图像编码的视觉专家、文本编码的语言专家和图像-文本融合的视觉-语言专家。由于建模的灵活性，我们可以在共享参数的情况下重复使用MOME变换器，用于不同的目的，即纯文本编码器、纯图像编码器和图像-文本融合编码器。

VLMO is jointly learned with three pre-training tasks, namely image-text contrastive learning, image-text matching, and masked language modeling. In addition, we propose a stagewise pre-training strategy to effectively leverage large-scale image-only and text-only corpus besides image-text pairs in VLMO pre-training. We first pretrain vision experts and self-attention modules of MOME Transformer on image-only data using masked image modeling proposed in BEIT [2]. We then pretrain language experts on text-only data using masked language modeling [10]. Finally, the model is used to initialize vision-language pre-training. By getting rid of the limited size of image-text pairs and their simple and short captions, stagewise pre-training on large amounts of image-only and text-only data helps VLMO to learn more generalizable representations.

VLMO是与三个预训练任务联合学习的，即图像-文本对比学习、图像-文本匹配和屏蔽语言建模。此外，我们提出了一个分阶段的预训练策略，以有效地利用VLMO预训练中除图像-文本对之外的大规模纯图像和纯文本语料。我们首先使用BEIT[2]中提出的遮蔽图像建模，在仅有图像的数据上预训练MOME Transformer的视觉专家和自我注意模块。然后，我们使用掩膜语言建模[10]对纯文本数据进行语言专家预训练。最后，该模型被用来初始化视觉-语言预训练。通过摆脱图像-文本对的有限规模及其简单而简短的标题，在大量的纯图像和纯文本数据上进行分阶段的预训练，帮助VLMO学习更多的可归纳的表示。

>NLP随着数据越大，性能越好，视觉上不明显，多模态含文本所以数据越大，性能越好。【CLIP证明这点】

Experimental results demonstrate that VLMO achieves state-of-the-art results on vision-language retrieval and classification tasks. Our model, used as a dual encoder, outperforms fusion-encoder-based models [3, 14, 20, 23] while enjoying a much faster inference speed on retrieval tasks. Moreover, our model also achieves state-of-the-art results on visual question answering (VQA) and natural language for visual reasoning (NLVR2), where VLMO is used as a fusion encoder.

实验结果表明，VLMO在视觉语言检索和分类任务上取得了最先进的成果。我们的模型作为双编码器使用，优于基于融合编码器的模型[3, 14, 20, 23]，同时在检索任务中享有更快的推理速度。此外，我们的模型在视觉问题回答（VQA）和视觉推理的自然语言（NLVR2）方面也取得了最先进的结果，其中VLMO被用作融合编码器

Our main contributions are summarized as follows:

• We propose a unified vision-language pretrained model VLMO that can be used as a fusion encoder for classification tasks, or fine-tuned as a dual encoder for retrieval tasks.
• We introduce a general-purpose multimodal Transformer for vision-language tasks, namely MOME Transformer, to encode different modalities. It captures modality-specific information by modality experts, and aligns contents of different modalities by the self-attention module shared across modalities.
• We show that stagewise pre-training using large amounts of image-only and text-only data greatly improves our vision-language pretrained model.

>【Transformer用了最少的先验假设，不挑输入，大一统】
>64张v100训练2天
>后续工作BEiT v3
>在视觉上训练好的自注意模型也能用在文本，但是反过来可以

In the future, we would like to work on improving VLMO from the following perspectives:
• We will scale up the model size used in VLMO pre-training. 【BeiR v3 ViT-G】
• We are also interested in fine-tuning VLMO for vision-language generation tasks, such as image captioning, following the method proposed in UniLM [11].【Vl-BeiT Beit V3需要解码器做图像字幕】
• We are going to explore to what extent vision-language pre-training can help each other modality, especially as the shared MOME backbone naturally blends in text and image representations.【Beit V3】【多模态有利于单模态】
• We can extend the proposed model to integrate more modalities (e.g., speech【WAVLM】, video, and structured knowledge{Layer LM V1 V2 V3}), supporting general-purpose multimodal pre-training.【MetaLM】【General-purpose多模态学习】

BEIT【21.6】 VLMO【21.11】 VL-BeiT【22.6】 BeiTV2【22.8】 BeitV3【08】

## 基于编码器和解码器的工作

### BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi

Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.

视觉语言预训练 (VLP) 提高了许多视觉语言任务的性能。 然而，大多数现有的预训练模型仅在基于理解的任务或基于生成的任务中表现出色。 此外，通过使用从网络收集的嘈杂图像文本对扩展数据集，性能的提高在很大程度上已经实现，这是次优的监督来源。 在本文中，我们提出了 BLIP，这是一种新的 VLP 框架，可以灵活地迁移到视觉语言理解和生成任务。 BLIP 通过引导字幕有效地利用了嘈杂的网络数据，字幕生成器生成合成字幕，过滤器去除嘈杂的字幕。 我们在广泛的视觉语言任务上取得了最先进的结果，例如图像文本检索（平均召回率+2.7%@1）、图像字幕（CIDEr 中+2.8%）和 VQA（ VQA 得分 +1.6%）。 当以零样本的方式直接转移到视频语言任务时，BLIP 也表现出强大的泛化能力。 代码、模型和数据集在此 https URL 上发布。

>用嘈杂的数据训练一个模型，然后用干净的数据继续训练，训练出一个比较好的模型
>统一视觉语言理解和生成

Vision-language pre-training has recently received tremen-dous success on various multimodal downstream tasks. However, existing methods have two major limitations:

(l)	Model perspective: most methods either adopt an encoder-based model (Radford et al., 2021; Li et al., 2021a), or an encoder-decoder (Cho et al., 2021; Wang et al., 2021) model. However, encoder-based models are less straightforward to directly transfer to text generation tasks (e.g. image captioning), whereas encoder-decoder models have not been successfully adopted for image-text retrieval tasks.

(2)	Data perspective: most state-of-the-art methods (e.g., CLIP (Radford et al., 2021), ALBEF (Li et al., 2021a), SimVLM (Wang et al., 2021)) pre-train on image-text pairs collected from the web. Despite the performance gain obtained by scaling up the dataset, our paper shows that the noisy web text is suboptimal for vision-language learning. 

To this end, we propose BLIP: Bootstrapping Language-Image Pre-training for unified vision-language understand-ing and generation. BLIP is a new VLP framework which enables a wider range of downstream tasks than existing methods. It introduces two contributions from the model and data perspective, respectively:

(a)	Multimodal mixture of Encoder-Decoder (MED): a new model architecture for effective multi-task pre-training and flexible transfer learning. An MED can operate either as a unimodal encoder, or an image-grounded text encoder, or an image-grounded text decoder. The model is jointly pre-trained with three vision-language objectives: image-text contrastive learning, image-text matching, and image-conditioned language modeling.

(b)	Captioning and Filtering (CapFilt): a new dataset boostrapping method for learning from noisy image-text pairs. We finetune a pre-trained MED into two modules: a captioner to produce synthetic captions given web images, and a filter to remove noisy captions from both the original web texts and the synthetic texts.
We perform extensive experiments and analysis, and make the following key observations.

(c)	We show that the captioner and the filter work together to achieve substantial performance improvement on various downstream tasks by bootstrapping the captions. We also find that more diverse captions yield larger gains.

(d)	BLIP achieves state-of-the-art performance on a wide range of vision-language tasks, including image-text retrieval, image captioning, visual question answering, visual reasoning, and visual dialog. We also achieve state-of- the-art zero-shot performance when directly transferring our models to two video-language tasks: text-to-video retrieval and videoQA.

视觉语言预训练最近在各种多模态的下游任务上获得了巨大的成功。然而，现有的方法有两个主要限制。

(l) 模型角度：大多数方法要么采用基于编码器的模型（Radford等人，2021；Li等人，2021a【ClIP ALBF】），要么采用编码器-解码器（Cho等人，2021；Wang等人，2021）【SilmVLM】模型。然而，基于编码器的模型不太容易直接转移到文本生成任务（如图像标题），而编码器-解码器模型还没有成功地用于图像-文本检索任务。【只有解码器不容易做生成任务，但编码器-解码器不好用于检索任务】

(2）数据角度：大多数最先进的方法（例如CLIP（Radford等人，2021年）、ALBEF（Li等人，2021年a）、SimVLM（Wang等人，2021年））对从网络上收集的图像-文本对进行预训练。从网络上收集的图像-文本对进行预训练。尽管通过扩大数据集获得了性能上的提高，但我们的论文显示，有噪声的网络文本对于视觉语言学习来说是次优的。【数据嘈杂】

为此，我们提出了BLIP：用于统一视觉语言理解和生成的引导语言-图像预训练。BLIP是一个新的VLP框架，与现有的方法相比，它能使下游任务的范围更广。它分别从模型和数据的角度引入了两个贡献。

(a) 编码器-解码器的多模态混合物（MED）：一个新的模型架构，用于有效的多任务预训练和灵活的转移学习。一个MED可以作为一个单模态编码器，或一个以图像为基础的文本编码器，或一个以图像为基础的文本解码器运行。该模型与三个视觉语言目标联合进行预训练：图像-文本对比学习、图像-文本匹配和图像条件下的语言建模。

(b) 标题和过滤（CapFilt）：一个新的数据集booststrapping方法，用于从噪声图像-文本对中学习。我们将预训练的MED微调为两个模块：一个是给定网络图像产生合成字幕的字幕员，另一个是去除原始网络文本和合成文本中的噪声字幕的过滤器。我们进行了广泛的实验和分析，并提出了以下主要意见。

(c) 我们表明，字幕机和过滤器一起工作，通过引导字幕，在各种下游任务上实现了实质性的性能改进。我们还发现，更多样化的字幕会产生更大的收益。

(d) BLIP在广泛的视觉语言任务上实现了最先进的性能，包括图像-文本检索、图像字幕、视觉问题回答、视觉推理和视觉对话。当直接将我们的模型转移到两个视频语言任务中时，我们也取得了最先进的零失误性能：文本到视频检索和视频QA。


>BLIP是个很好的工具

### CoCa: Contrastive Captioners are Image-Text Foundation Models

Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu

Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.

探索大规模的预训练基础模型在计算机视觉中具有重要意义，因为这些模型可以快速转移到许多下游任务中。本文介绍了Contrastive Captioner（CoCa），这是一个极简的设计，将图像-文本编码器-解码器基础模型与对比性损失和字幕损失联合起来进行预训练，从而将来自CLIP等对比性方法和SimVLM等生成性方法的模型能力归纳起来。与标准的编码器-解码器转化器相比，所有的解码器层都关注编码器的输出，CoCa在解码器层的前半部分省略了交叉关注，以编码单模态的文本表示，并将其余的解码器层级联起来，交叉关注图像编码器的多模态图像-文本表示。我们在单模态图像和文本嵌入之间应用了对比性损失，此外还在多模态解码器输出上应用了字幕损失，该损失可自动预测文本标记。通过共享相同的计算图，这两个训练目标的计算效率很高，开销最小。CoCa在网络规模的alt-text数据和有注释的图像上进行了端到端的预训练，将所有标签简单地视为文本，无缝地将自然语言监督与表示学习统一起来。从经验上看，CoCa在广泛的下游任务上实现了最先进的性能，包括视觉识别（ImageNet、Kinetics-400/600/700、Moments-in-Time）、跨模式检索（MSCOCO、Flickr30K、MSR-VTT）、多模式理解（VQA、SNLI-VE、NLVR2）和图像说明（MSCOCO、NoCaps）。值得注意的是，在ImageNet的分类上，CoCa获得了86.3%的零次顶级准确率，使用冻结的编码器和学习的分类头获得了90.6%的准确率，使用微调的编码器在ImageNet上获得了最新的91.0%顶级准确率。

> ALBF后续工作
> attentional pooling是可学的，不同的pooling可使用到各种各样的任务
> 没用ITM是为了解决效率问题

### Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks

Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei

A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).

语言、视觉和多模态预训练的大融合正在出现。在这项工作中，我们引入了一个通用的多模态基础模型BEiT-3，它在视觉和视觉-语言任务上都达到了最先进的转移性能。具体来说，我们从三个方面推进大融合：骨干架构、预训练任务和模型扩展。我们引入了用于通用建模的多路变换器，其中的模块化架构可以实现深度融合和特定模式的编码。基于共享骨架，我们以统一的方式对图像（Imglish）、文本（English）和图像-文本对（"平行句"）进行掩蔽的 "语言 "建模。实验结果表明，BEiT-3在物体检测（COCO）、语义分割（ADE20K）、图像分类（ImageNet）、视觉推理（NLVR2）、视觉问题回答（VQAv2）、图像说明（COCO）和跨模式检索（Flickr30K，COCO）方面获得了最先进的性能。

>把图像也看成一种语言
>用mask函数充当唯一的损失函数

#### 1 Introduction: The Big Convergence

Recent years have featured a trend toward the big convergence of language [RNSS18, DCLT19, DYW+19], vision [BDPW22, PDB+22], and multimodal [WBDW21, RKH+21, YWV+22] pre¬training. By performing large-scale pretraining on massive data, we can easily transfer the models to various downstream tasks. It is appealing that we can pretrain a general-purpose foundation model that handles multiple modalities. In this work, we advance the convergence trend for vision-language pretraining from the following three aspects.

First, the success of Transformers [VSP+17] is translated from language to vision [DBK+20] and multimodal [KSK21, WBDW21] problems. The unification of network architectures enables us to seamlessly handle multiple modalities. For vision-language modeling, there are various ways to apply Transformers due to the different natures of downstream tasks. For example, the dual-encoder architecture is used for efficient retrieval [RKH+21], encoder-decoder networks for generation tasks [WYY+21], and the fusion-encoder architecture for image-text encoding [KSK21]. However, most foundation models have to manually convert the end-task formats according to the specific architectures. Moreover, the parameters are usually not effectively shared across modalities. In this work, we adopt Multiway Transformers [WBDW21] for general-purpose modeling, i.e., one unified architecture shared for various downstream tasks. The modular network also comprehensively considers modality-specific encoding and cross-modality fusion.

Second, the pretraining task based on masked data modeling has been successfully applied to various modalities, such as texts [DCLT19], images [BDPW22, PDB+22], and image-text pairs [BWDW22]. Current vision-language foundation models usually multitask other pretraining objectives (such as image-text matching), rendering scaling-up unfriendly and inefficient. In contrast, we only use one pretraining task, i.e., mask-then-predict, to train a general-purpose multimodal foundation model. By regarding the image as a foreign language (i.e., Imglish), we handle texts and images in the same manner without fundamental modeling differences. Consequentially, image-text pairs are utilized as “parallel sentences” in order to learn the alignments between modalities. We also show that the simple yet effective method learns strong transferable representations, achieving state-of-the-art performance on both vision and vision-language tasks. The prominent success demonstrates the superiority of generative pretraining [DCLT19, BDPW22].

Third, scaling up the model size and data size universally improves the generalization quality of foundation models, so that we can transfer them to various downstream tasks. We follow the philosophy and scale up the model size to billions of parameters. Moreover, we scale up the pretraining data size in our experiments while only using publicly accessible resources for academic reproducibility. Although without using any private data, our method outperforms state-of-the-art foundation models that rely on in-house data by a decent margin. In addition, the scaling up benefits from treating images as a foreign language, as we can directly reuse the pipeline developed for large-scale language model pretraining.

In this work, we take advantage of the above ideas to pretrain a general-purpose multimodal foundation model BEIT-3. We pretrain a Multiway Transformer by performing masked data modeling on images, texts, and image-text pairs. During pretraining, we randomly mask some proportion of text tokens or image patches. The self-supervised learning objective is to recover the original tokens (i.e., text tokens, or visual tokens) given corrupted inputs. The model is general-purpose in the sense that it can be repurposed for various tasks regardless of input modalities, or output formats.

As shown in Figure 1 and Table 1, BEIT-3 achieves state-of-the-art transfer performance across a broad range of vision and vision-language tasks. We evaluate BEIT-3 on extensive downstream tasks and datasets, i.e., object detection (COCO), instance segmentation (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO). Specifically, our model outperforms previous strong foundation models [YWV+22, ADL+22, YCC+21] despite that we only use public resources for pretraining and finetuning. The model also obtains better results than specialized models. Moreover, BEIT-3 not only performs well on vision-language tasks but also on vision tasks (such as object detection, and semantic segmentation).

近年来，语言[RNSS18, DCLT19, DYW+19]、视觉[BDPW22, PDB+22]和多模态[WBDW21, RKH+21, YWV+22]预训练出现了大融合的趋势。通过对海量数据进行大规模的预训练，我们可以很容易地将模型转移到各种下游任务中。我们可以预训练一个处理多种模式的通用基础模型，这一点很有吸引力。在这项工作中，我们从以下三个方面推进视觉语言预训练的融合趋势。

首先，Transformers[VSP+17]的成功被从语言转化为视觉[DBK+20]和多模态[KSK21, WBDW21]问题。网络架构的统一使我们能够无缝地处理多种模式。对于视觉-语言建模，由于下游任务的不同性质，有各种方法来应用变形器。例如，双编码器架构用于高效检索[RKH+21]，编码器-解码器网络用于生成任务[WEY+21]，以及融合编码器架构用于图像-文本编码[KSK21]。然而，大多数基础模型必须根据具体的架构手动转换终端任务的格式。此外，参数通常不能有效地跨模态共享。在这项工作中，我们采用多路变压器[WBDW21]进行通用建模，即为各种下游任务共享一个统一的架构。模块化的网络还全面考虑了特定模态的编码和跨模态的融合。

其次，基于遮蔽数据建模的预训练任务已经成功应用于各种模态，如文本[DCLT19]【Bert】、图像[BDPW22, PDB+22]【Beit】和图像-文本对[BWDW22]【VlBEIT】。目前的视觉语言基础模型通常会同时处理其他预训练目标（如图像-文本匹配），使得扩展变得不友好和低效率。相比之下，我们只使用一个预训练任务，即 "掩蔽-然后预测"，来训练一个通用的多模态基础模型。通过将图像视为一种外语（即Imglish），我们以相同的方式处理文本和图像，没有根本的建模差异。因此，图像-文本对被用作 "平行句子"，以学习不同模式之间的排列组合。我们还表明，这种简单而有效的方法能够学习到强大的可转移表征，在视觉和视觉-语言任务上都取得了最先进的表现。这一突出的成功证明了生成性预训练的优越性[DCLT19, BDPW22]。

第三，扩大模型规模和数据规模普遍提高了基础模型的泛化质量，因此我们可以将其转移到各种下游任务中。我们遵循这一理念，将模型规模扩大到数十亿的参数。此外，我们在实验中扩大了预训练数据的规模，同时只使用公开的资源以保证学术上的可重复性。尽管没有使用任何私人数据，我们的方法还是以相当大的幅度超过了依赖内部数据的最先进的基础模型。此外，扩大规模还得益于将图像作为一种外语，因为我们可以直接重复使用为大规模语言模型预训练开发的管道。

在这项工作中，我们利用上述想法来预训练一个通用的多模态基础模型BEIT-3。我们通过对图像、文本和图像-文本对进行遮蔽数据建模来预训练多路转换器。在预训练过程中，我们随机地掩盖一些比例的文本标记或图像补丁。自我监督的学习目标是恢复原始标记（即文本标记，或视觉标记），给定损坏的输入。该模型是通用的，因为它可以被重新用于各种任务，而不考虑输入模式或输出格式。

如图1和表1所示，BEIT-3在广泛的视觉和视觉-语言任务中实现了最先进的转移性能。我们在广泛的下游任务和数据集上评估了BEIT-3，即物体检测（COCO）、实例分割（COCO）、语义分割（ADE20K）、图像分类（ImageNet）、视觉推理（NLVR2）、视觉问题回答（VQAv2）、图像说明（COCO）和跨模式检索（Flickr30K，COCO）。具体来说，我们的模型优于之前的强基础模型[YWV+22, ADL+22, YCC+21]，尽管我们只使用公共资源进行预训练和微调。该模型也获得了比专门模型更好的结果。此外，BEIT-3不仅在视觉-语言任务上表现良好，而且在视觉任务（如物体检测和语义分割）上也表现良好。

>未来是大一统的多模态的模型
