---
title: 【论文】GPT，GPT-2，GPT-3 论文精读
date: 2022-11-21 07:00:00 +/-0800
categories: [论文,深度学习]
tags: [深度学习,论文]     # TAG names should always be lowercase 标记名称应始终为小写
math: true
---

<https://gpt3demo.com/>

<!-- ```mermaid

``` -->
## Improving Language Understanding by Generative Pre-Training 使用通用的预训练提高语言理解能力（GPT）

|Alec Radford|Karthik Narasimhan|Tim Salimans|Ilya Sutskever|
|---|---|---|---|
|OpenAI|OpenAI|OpenAI|OpenAI|
|alec@openai.com|karthikn@openai.com|tim@openai.com|ilyasu@openai.com|

### Abstract 摘要

Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).

自然语言理解包括范围广泛的不同任务，例如文本蕴涵、问答、语义相似性评估和文档分类。尽管大量未标记的文本语料库很丰富，但用于学习这些特定任务的标记数据却很少，这使得经过判别训练的模型很难有充分的表现。我们证明，通过在不同的无标签文本语料库上对语言模型进行生成性预训练，然后在每个具体任务上进行分辨性微调，可以实现这些任务的巨大收益。与以前的方法相比，我们在微调期间使用任务感知的输入转换【构造和任务相关的输入】，以实现有效的迁移，同时对模型体系结构进行最小的更改。我们证明了我们的方法在自然语言理解的广泛基准上的有效性。我们的通用任务不可知模型优于使用专门为每个任务设计的架构的判别训练模型，在所研究的 12 个任务中的 9 个中显着提高了现有技术。例如，我们在常识推理（Stories Cloze Test）上实现了 8.9% 的绝对改进，在问答（RACE）上实现了 5.7% 的绝对改进，在文本蕴涵（MultiNLI）上实现了 1.5% 的绝对改进。

#### 1 Introduction 导言

The ability to learn effectively from raw text is crucial to alleviating the dependence on supervised learning in natural language processing (NLP). Most deep learning methods require substantial amounts of manually labeled data, which restricts their applicability in many domains that suffer from a dearth of annotated resources [61]. In these situations, models that can leverage linguistic information from unlabeled data provide a valuable alternative to gathering more annotation, which can be time-consuming and expensive. Further, even in cases where considerable supervision is available, learning good representations in an unsupervised fashion can provide a significant performance boost. The most compelling evidence for this so far has been the extensive use of pre-trained word embeddings [10,39,42] to improve performance on a range of NLP tasks [8,11,26,45].

从原始文本中有效学习的能力对于缓解自然语言处理（NLP）中对监督学习的依赖至关重要。大多数深度学习方法需要大量的手动标记数据，这限制了它们在许多缺乏注释资源的领域的适用性[61]。在这些情况下，能够利用未标记数据的语言信息的模型为收集更多的注释提供了一个有价值的选择，而这可能是费时和昂贵的。此外，即使在有相当多的监督的情况下，以无监督的方式学习好的表征也可以提供一个显著的性能提升。到目前为止，最令人信服的证据是广泛使用预训练的词嵌入[10,39,42]来提高一系列NLP任务的性能[8,11,26,45]

Leveraging more than word-level information from unlabeled text, however, is challenging for two main reasons. First, it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer. Recent research has looked at various objectives such as language modeling [44], machine translation [38], and discourse coherence [22], with each method outperforming the others on different tasks.$^1$  Second, there is no consensus on the most effective way to transfer these learned representations to the target task. Existing techniques involve a combination of making task-specific changes to the model architecture [43,44], using intricate learning schemes [21] and adding auxiliary learning objectives [50]. These uncertainties have made it difficult to develop effective semi-supervised learning approaches for language processing.

然而，利用来自未标记文本的单词级信息以外的信息具有挑战性，主要有两个原因。 首先，尚不清楚哪种类型的优化目标在学习对迁移有用的文本表示方面最有效。 【不知道用什么目标函数】最近的研究着眼于各种目标，例如语言建模 [44]、机器翻译 [38] 和语篇连贯性 [22]，每种方法在不同任务上的表现都优于其他方法。$^1$ 其次，关于 将这些学习到的表示转移到目标任务的最有效方法。【不知道如何将学到的文本表示传递到下游的子任务上面】 现有技术包括对模型架构进行特定于任务的更改 [43,44]、使用复杂的学习方案 [21] 和添加辅助学习目标 [50]。 这些不确定性使得开发有效的语言处理半监督学习方法变得困难。

>$^1$ <https://gluebenchmark.com/leaderboard/>

In this paper, we explore a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning. Our goal is to learn a universal representation that transfers with little adaptation to a wide range of tasks. We assume access to a large corpus of unlabeled text and several datasets with manually annotated training examples (target tasks). Our setup does not require these target tasks to be in the same domain as the unlabeled corpus. We employ a two-stage training procedure. First, we use a language modeling objective on the unlabeled data to learn the initial parameters of a neural network model. Subsequently, we adapt these parameters to a target task using the corresponding supervised objective.

本文探索了一种结合无监督预训练和监督微调的半监督语言理解任务方法。【半监督：有一些标号的数据，然后想方设法利用没有标好的数据】我们的目标是学习一种通用的表示，这种表示经过很小的微调后可以适应各种任务。我们假设通过手动注释的训练示例（目标任务）访问大量未标记文本和多个数据集。我们的设置不要求这些目标任务与未标记的文集在同一个域中。我们采用两阶段训练程序。首先，我们使用未标记数据上的语言模型对象去学习神经网络模型的初始参数。随后，我们使用相应的监督目标对这些参数进行调整，以适应目标任务。

For our model architecture, we use the Transformer [62], which has been shown to perform strongly on various tasks such as machine translation [62], document generation [34], and syntactic parsing [29]. This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks. During transfer, we utilize task-specific input adaptations derived from traversal-style approaches [52], which process structured text input as a single contiguous sequence of tokens. As we demonstrate in our experiments, these adaptations enable us to fine-tune effectively with minimal changes to the architecture of the pre-trained model.

对于我们的模型架构，我们使用了 Transformer [62]，它已被证明在机器翻译 [62]、文档生成 [34] 和句法解析 [29] 等各种任务中表现出色。 与循环网络等替代方案相比，这种模型选择为我们提供了更结构化的记忆来处理文本中的长期依赖关系，从而在不同任务中产生稳健的迁移性能。 在迁移过程中，我们利用源自遍历式方法 [52] 的特定于任务的输入自适应【与任务相关的输入】，该方法将结构化文本输入处理为单个连续的标记序列。 正如我们在实验中所展示的那样，这些调整使我们能够在对预训练模型的架构进行最小更改的情况下有效地进行微调。

We evaluate our approach on four types of language understanding tasks - natural language inference, question answering, semantic similarity, and text classification. Our general task-agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on the recently introduced GLUE multi-task benchmark [64]. We also analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks.

我们在四种类型的语言理解任务上评估我们的方法——自然语言推理、问答、语义相似性和文本分类。 我们的通用任务不可知模型优于采用专门为每项任务设计的架构的判别式训练模型，显着改进了所研究的 12 项任务中的 9 项的最新技术水平。 例如，我们在常识推理（故事完形填空测试）[40]、问题回答（RACE）[30]、文本蕴含（MultiNLI）[66] 和 5.5% 方面实现了 8.9% 的绝对改进。 最近介绍了 GLUE 多任务基准 [64]。 我们还分析了预训练模型在四种不同设置下的零样本行为，并证明它为下游任务获取了有用的语言知识。

#### 2 Related Work 相关工作

**Semi-supervised learning for NLP** Our work broadly falls under the category of semi-supervised learning for natural language. This paradigm has attracted significant interest, with applications to tasks like sequence labeling [24,33,57] or text classification [41,70]. The earliest approaches used unlabeled data to compute word-level or phrase-level statistics, which were then used as features in a supervised model [33]. Over the last few years, researchers have demonstrated the benefits of using word embeddings [11,39,42], which are trained on unlabeled corpora, to improve performance on a variety of tasks [8,11,26,45]. These approaches, however, mainly transfer word-level information, whereas we aim to capture higher-level semantics.

Recent approaches have investigated learning and utilizing more than word-level semantics from unlabeled data. Phrase-level or sentence-level embeddings, which can be trained using an unlabeled corpus, have been used to encode text into suitable vector representations for various target tasks [28,32,1,36,22,12,56,31].

**Unsupervised pre-training** Unsupervised pre-training is a special case of semi-supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective. Early works explored the use of the technique in image classification [20, 49, 63] and regression tasks [3]. Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks. In recent work, the method has been used to help train deep neural networks on various tasks like image classification [69], speech recognition [68], entity disambiguation [17] and machine translation [48].

The closest line of work to ours involves pre-training a neural network using a language modeling objective and then fine-tuning it on a target task with supervision. Dai et al. [13] and Howard and Ruder [21] follow this method to improve text classification. However, although the pre-training phase helps capture some linguistic information, their usage of LSTM models restricts their prediction ability to a short range. In contrast, our choice of transformer networks allows us to capture longer- range linguistic structure, as demonstrated in our experiments. Further, we also demonstrate the effectiveness of our model on a wider range of tasks including natural language inference, paraphrase detection and story completion. Other approaches [43, 44, 38] use hidden representations from a pre-trained language or machine translation model as auxiliary features while training a supervised model on the target task. This involves a substantial amount of new parameters for each separate target task, whereas we require minimal changes to our model architecture during transfer.

**Auxiliary training objectives** Adding auxiliary unsupervised training objectives is an alternative form of semi-supervised learning. Early work by Collobert and Weston [10] used a wide variety of auxiliary NLP tasks such as POS tagging, chunking, named entity recognition, and language modeling to improve semantic role labeling. More recently, Rei [50] added an auxiliary language modeling objective to their target task objective and demonstrated performance gains on sequence labeling tasks. Our experiments also use an auxiliary objective, but as we show, unsupervised pre-training already learns several linguistic aspects relevant to target tasks.

### 3 Framework 框架

Our training procedure consists of two stages. The first stage is learning a high-capacity language model on a large corpus of text. This is followed by a fine-tuning stage, where we adapt the model to a discriminative task with labeled data.

#### 3.1 Unsupervised pre-training

Given an unsupervised corpus of tokens $\mathcal{U}=\{u_1,...,u_n\}$, we use a standard language modeling objective to maximize the following likelihood:

给定一个无监督的标记语料库 $\mathcal{U}=\{u_1,...,u_n\}$，我们使用标准语言建模目标来最大化以下可能性：【每个词表现为$u_i$，所有$u_i$的集合就是整个文本】

$$
L_1(\mathcal{U}) = \sum_i log P(u_i|u_{i-k},...,u_{i-1};\Theta) \tag{1}
$$

【预测第i个词出现的概率，即每次拿$k$个词，预测下个词是什么，$\Theta$是模型】

where $k$ is the size of the context window, and the conditional probability $P$ is modeled using a neural network with parameters $\Theta$. These parameters are trained using stochastic gradient descent [51].

其中 $k$ 是上下文窗口的大小，条件概率 $P$ 是使用参数为 $\Theta$ 的神经网络建模的。 这些参数使用随机梯度下降 [51] 进行训练。

In our experiments, we use a multi-layer Transformer decoder [34] for the language model, which is a variant of the transformer [62]. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:

在我们的实验中，我们为语言模型使用多层 Transformer 解码器 [34]，它是 transformer [62] 的变体。 该模型对输入上下文标记应用多头自注意力操作，然后是位置前馈层，以在目标标记上产生输出分布：【与解码器不同，对k个元素，模型只能看到这k个元素，而编码器能看到所有的元素】

$$
\begin{align*}
h_0 &= UW_e + W_p \\
h_l &= \mathbf{transformer_block}(h_{l—1}) \forall i \in [1,n] \tag{2} \\
P(u) &= \mathbf{softmax}(h_nW_e^T )
\end{align*}
$$

where $U = (u_{-k},...,u_{-1})$ is the context vector of tokens, $n$ is the number of layers, $W_e$ is the token embedding matrix, and $W_p$ is the position embedding matrix.

其中$U = (u_{-k},...,u_{-1})$是token的上下文向量，$n$是层数，$W_e$是token嵌入矩阵，$W_p $是位置嵌入矩阵。

【预测未来难度高于完形填空】

#### 3.2 Supervised fine-tuning 监督微调

After training the model with the objective in Eq. 1, we adapt the parameters to the supervised target task. We assume a labeled dataset $\mathcal{C}$, where each instance consists of a sequence of input tokens, $x^1,...,x^m$, along with a label $y$. The inputs are passed through our pre-trained model to obtain the final transformer block’s activation $h_l^m$, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$:

在使用等式中的目标训练模型之后。 1，我们调整参数以适应监督目标任务。 我们假设一个带标签的数据集 $\mathcal{C}$，其中每个实例都由一系列输入标记 $x^1,...,x^m$ 以及标签 $y$ 组成。 输入通过我们预训练的模型来获得最终变换器块的激活 $h_l^m$，然后将其馈送到带有参数 $W_y$ 的附加线性输出层以预测 $y$：

【假设监督数据集合$\mathcal{从}$的输入$X$是一个序列 $x^1,...,x^m$ ，输出是一个分类$y$的标签。把 $x^1,...,x^m$ $输入 Transformer 模型，得到最上层最后一个时刻的输出$h_l^m$，将其通过我们新增的一个 Softmax 层（参数为$W_y$）进行分类，最后用交叉熵计算损失，从而根据标准数据调整 Transformer 的参数以及 Softmax 的参数$W_y$。这等价于最大似然估计：】

$$
P(y|x^1,...,x^m) = \mathbf{softmax}(h_l^mW_y ) \tag{3}
$$

This gives us the following objective to maximize:

这为我们提供了以下最大化目标：

$$
L_2(\mathcal{C})=\sum_{(x,y)}\log P(y|x^1,...,x^m) \tag{4}
$$

We additionally found that including language modeling as an auxiliary objective to the fine-tuning helped learning by (a) improving generalization of the supervised model, and (b) accelerating convergence. This is in line with prior work [50,43], who also observed improved performance with such an auxiliary objective. Specifically, we optimize the following objective (with weight $\lambda$):

我们还发现，将语言建模作为微调的辅助目标通过 (a) 改进监督模型的泛化，以及 (b) 加速收敛来帮助学习。 这与之前的工作 [50,43] 一致，后者还观察到这种辅助目标的性能有所提高。 具体来说，我们优化了以下目标（权重 $\lambda$）：【两个目标函数：1. 给整个序列预测序列对应的标签 2.给每个词，预测下一个词】

$$
L_3(C) =L-2(\mathcal{C})+ \lambda *L_1(\mathcal{C}) \tag{5}
$$

Overall, the only extra parameters we require during fine-tuning are $W_y$, and embeddings for delimiter tokens (described below in Section 3.3).

总的来说，我们在微调期间唯一需要的额外参数是 $W_y$ 和分隔符标记的嵌入（在下面的 3.3 节中描述）。

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-1/Fig%201.png"/></div>

Figure 1: **(left)** Transformer architecture and training objectives used in this work. **(right)** Input transformations for fine-tuning on different tasks. We convert all structured inputs into token sequences to be processed by our pre-trained model, followed by a linear+softmax layer.

>包括
>[start]、[extract]、[delim]是三个特殊的次元，[start]表示开始，[extract]表示抽取，亦可以认为是结束符号
>- 分类任务
>   前面加一个开始的词元后面加一个抽取的词元，扔到模型里，输出然后分类
>- 蕴含任务
>- 相似任务
>   做两次是因为两个文本相似是一个对称关系
>- 多选题

#### 3.3 Task-specific input transformations

For some tasks, like text classification, we can directly fine-tune our model as described above. Certain other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers. Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks. Previous work proposed learning task specific architectures on top of transferred representations [44]. Such an approach re-introduces a significant amount of task-specific customization and does not use transfer learning for these additional architectural components. Instead, we use a traversal-style approach [52], where we convert structured inputs into an ordered sequence that our pre-trained model can process. These input transformations allow us to avoid making extensive changes to the architecture across tasks. We provide a brief description of these input transformations below and Figure 1 provides a visual illustration. All transformations include adding randomly initialized start and end tokens $(\left\langle s\right\rangle,\left\langle e\right\rangle)$.

**Textual entailment** For entailment tasks, we concatenate the premise p and hypothesis h token sequences, with a delimiter token ($) in between.

**Similarity** For similarity tasks, there is no inherent ordering of the two sentences being compared. To reflect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations $h_l^m$ which are added element-wise before being fed into the linear output layer.

**Question Answering and Commonsense Reasoning** For these tasks, we are given a context document $z$, a question $q$, and a set of possible answers $\{a_k\}$. We concatenate the document context and question with each possible answer, adding a delimiter token in between to get $[z;q;\$;a_k]$. Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers.

### 4 Experiments 实验

#### 4.1 Setup

**Unsupervised pre-training** We use the BooksCorpus dataset [71] for training the language model. It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance. Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size but is shuffled at a sentence level - destroying long-range structure. Our language model achieves a very low token level perplexity of 18.4 on this corpus.

**无监督预训练** 我们使用 BooksCorpus 数据集 [71] 来训练语言模型。 它包含 7,000 多本独特的未出版书籍，这些书籍来自各种类型，包括冒险、奇幻和浪漫。 至关重要的是，它包含很长一段连续的文本，这使得生成模型能够学习以远程信息为条件。 另一个数据集 1B Word Benchmark 被类似的方法 ELMo [44] 使用，其大小大致相同，但在句子级别进行了打乱——破坏了远程结构。 我们的语言模型在这个语料库上达到了 18.4 的非常低的标记级别困惑。

Table 1: A list of the different tasks and datasets used in our experiments.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-1/Table%201.png"/></div>

**Model specifications** Our model largely follows the original transformer work [62]. We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm [2] is used extensively throughout the model, a simple weight initialization of $N (0,0.02)$ was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in [37], with $w = 0.01$ on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We used learned position embeddings instead of the sinusoidal version proposed in the original work. We use the $ftfy$ library$^2$  to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer$^3$.

**模型规格** 我们的模型很大程度上遵循原始变压器工作 [62]。我们训练了一个只有 12 层解码器的转换器，带有屏蔽的自注意力头（768 维状态和 12 个注意力头）。对于位置前馈网络，我们使用了 3072 维内部状态。我们使用 Adam 优化方案 [27]，最大学习率为 2.5e-4。在前 2000 次更新中，学习率从零开始线性增加，并使用余弦计划退火到 0。我们在 64 个随机抽样的小批量、512 个标记的连续序列上训练 100 个时期。由于 layernorm [2] 在整个模型中广泛使用，因此 $N (0,0.02)$ 的简单权重初始化就足够了。我们使用具有 40,000 次合并 [53] 的字节对编码 (BPE) 词汇表和 0.1 比率的残差、嵌入和注意力丢失进行正则化。我们还采用了 [37] 中提出的 L2 正则化的修改版本，在所有非偏差或增益权重上 $w = 0.01$。对于激活函数，我们使用了高斯误差线性单元 (GELU) [18]。我们使用学习位置嵌入而不是原始工作中提出的正弦版本。我们使用 $ftfy$ 库 $^2$ 清理 BooksCorpus 中的原始文本，标准化一些标点符号和空格，并使用 spaCy 分词器 $^3$。

>$^2$<https://ftfy.readthedocs.io/en/latest/>
$^3$<https://spacy.io/>

**Fine-tuning details** Unless specified, we reuse the hyperparameter settings from unsupervised pre-training. We add dropout to the classifier with a rate of 0.1. For most tasks, we use a learning rate of 6.25e-5 and a batchsize of 32. Our model finetunes quickly and 3 epochs of training was sufficient for most cases. We use a linear learning rate decay schedule with warmup over 0.2% of training. was set to 0.5.

#### 4.2 Supervised fine-tuning

We perform experiments on a variety of supervised tasks including natural language inference, question answering, semantic similarity, and text classification. Some of these tasks are available as part of the recently released GLUE multi-task benchmark [64], which we make use of. Figure 1 provides an overview of all the tasks and datasets.

**Natural Language Inference** The task of natural language inference (NLI), also known as recognizing textual entailment, involves reading a pair of sentences and judging the relationship between them from one of entailment, contradiction or neutral. Although there has been a lot of recent interest [58, 35, 44], the task remains challenging due to the presence of a wide variety of phenomena like lexical entailment, coreference, and lexical and syntactic ambiguity. We evaluate on five datasets with diverse sources, including image captions (SNLI), transcribed speech, popular fiction, and government reports (MNLI), Wikipedia articles (QNLI), science exams (SciTail) or news articles (RTE).

Table 2 details various results on the different NLI tasks for our model and previous state-of-the-art approaches. Our method significantly outperforms the baselines on four of the five datasets, achieving absolute improvements of upto 1.5% on MNLI, 5% on SciTail, 5.8% on QNLI and 0.6% on SNLI over the previous best results. This demonstrates our model’s ability to better reason over multiple sentences, and handle aspects of linguistic ambiguity. On RTE, one of the smaller datasets we evaluate on (2490 examples), we achieve an accuracy of 56%, which is below the 61.7% reported by a multi-task biLSTM model. Given the strong performance of our approach on larger NLI datasets, it is likely our model will benefit from multi-task training as well but we have not explored this currently. 

Table 2: Experimental results on natural language inference tasks, comparing our model with current state-of-the-art methods. 5x indicates an ensemble of 5 models. All datasets use accuracy as the evaluation metric.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-1/Table%202.png"/></div>

Table 3: Results on question answering and commonsense reasoning, comparing our model with current state-of-the-art methods.. 9x means an ensemble of 9 models.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-1/Table%203.png"/></div>

**Question answering and commonsense reasoning** Another task that requires aspects of single and multi-sentence reasoning is question answering. We use the recently released RACE dataset [30], consisting of English passages with associated questions from middle and high school exams. This corpus has been shown to contain more reasoning type questions that other datasets like CNN [19] or SQuaD [47], providing the perfect evaluation for our model which is trained to handle long-range contexts. In addition, we evaluate on the Story Cloze Test [40], which involves selecting the correct ending to multi-sentence stories from two options. On these tasks, our model again outperforms the previous best results by significant margins - up to 8.9% on Story Cloze, and 5.7% overall on RACE. This demonstrates the ability of our model to handle long-range contexts effectively.

**Semantic Similarity** Semantic similarity (or paraphrase detection) tasks involve predicting whether two sentences are semantically equivalent or not. The challenges lie in recognizing rephrasing of concepts, understanding negation, and handling syntactic ambiguity. We use three datasets for this task - the Microsoft Paraphrase corpus (MRPC) [14] (collected from news sources), the Quora Question Pairs (QQP) dataset [9], and the Semantic Textual Similarity benchmark (STS-B) [6]. We obtain state-of-the-art results on two of the three semantic similarity tasks (Table 4) with a 1 point absolute gain on STS-B. The performance delta on QQP is significant, with a 4.2% absolute improvement over Single-task BiLSTM + ELMo + Attn.

**Classification** Finally, we also evaluate on two different text classification tasks. The Corpus of Linguistic Acceptability (CoLA) [65] contains expert judgements on whether a sentence is grammatical or not, and tests the innate linguistic bias of trained models. The Stanford Sentiment Treebank (SST-2) [54], on the other hand, is a standard binary classification task. Our model obtains an score of 45.4 on CoLA, which is an especially big jump over the previous best result of 35.0, showcasing the innate linguistic bias learned by our model. The model also achieves 91.3% accuracy on SST-2, which is competitive with the state-of-the-art results. We also achieve an overall score of 72.8 on the GLUE benchmark, which is significantly better than the previous best of 68.9.

Table 4: Semantic similarity and classification results, comparing our model with current state-of-the- art methods. All task evaluations in this table were done using the GLUE benchmark. (mc= Mathews correlation, acc=Accuracy, pc=Pearson correlation)

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-1/Table%204.png"/></div>

Overall, our approach achieves new state-of-the-art results in 9 out of the 12 datasets we evaluate on, outperforming ensembles in many cases. Our results also indicate that our approach works well across datasets of different sizes, from smaller datasets such as STS-B (25.7k training examples)- to the largest one - SNLI ($\approx 550k$ training examples).

### 5 Analysis

**Impact of number of layers transferred** We observed the impact of transferring a variable number of layers from unsupervised pre-training to the supervised target task. Figure 2(left) illustrates the performance of our approach on MultiNLI and RACE as a function of the number of layers transferred. We observe the standard result that transferring embeddings improves performance and that each transformer layer provides further benefits up to 9% for full transfer on MultiNLI. This indicates that each layer in the pre-trained model contains useful functionality for solving target tasks.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-1/Fig%202.png"/></div>

Figure 2: (left) Effect of transferring increasing number of layers from the pre-trained language model on RACE and MultiNLI. (right) Plot showing the evolution of zero-shot performance on different tasks as a function of LM pre-training updates. Performance per task is normalized between a random guess baseline and the current state-of-the-art with a single model

**Zero-shot Behaviors** We’d like to better understand why language model pre-training of transformers is effective. A hypothesis is that the underlying generative model learns to perform many of the tasks we evaluate on in order to improve its language modeling capability and that the more structured attentional memory of the transformer assists in transfer compared to LSTMs. We designed a series of heuristic solutions that use the underlying generative model to perform tasks without supervised fine-tuning. We visualize the effectiveness of these heuristic solutions over the course of generative pre-training in Fig 2(right). We observe the performance of these heuristics is stable and steadily increases over training suggesting that generative pre-training supports the learning of a wide variety of task relevant functionality. We also observe the LSTM exhibits higher variance in its zero-shot performance suggesting that the inductive bias of the Transformer architecture assists in transfer.

Table 5: Analysis of various model ablations on different tasks. Avg. score is a unweighted average of all the results. (mc= Mathews correlation, acc=Accuracy, pc=Pearson correlation)

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-1/Table%205.png"/></div>

For CoLA (linguistic acceptability), examples are scored as the average token log-probability the generative model assigns and predictions are made by thresholding. For SST-2 (sentiment analysis), we append the token very to each example and restrict the language model’s output distribution to only the words positive and negative and guess the token it assigns higher probability to as the prediction. For RACE (question answering), we pick the answer the generative model assigns the highest average token log-probability when conditioned on the document and question. For DPRD [46] (winograd schemas), we replace the definite pronoun with the two possible referrents and predict the resolution that the generative model assigns higher average token log-probability to the rest of the sequence after the substitution.

**Ablation studies** We perform three different ablation studies (Table 5). First, we examine the performance of our method without the auxiliary LM objective during fine-tuning. We observe that the auxiliary objective helps on the NLI tasks and QQP. Overall, the trend suggests that larger datasets benefit from the auxiliary objective but smaller datasets do not. Second, we analyze the effect of the Transformer by comparing it with a single layer 2048 unit LSTM using the same framework. We observe a 5.6 average score drop when using the LSTM instead of the Transformer. The LSTM only outperforms the Transformer on one dataset - MRPC. Finally, we also compare with our transformer architecture directly trained on supervised target tasks, without pre-training. We observe that the lack of pre-training hurts performance across all the tasks, resulting in a 14.8% decrease compared to our full model.

### 6 Conclusion

We introduced a framework for achieving strong natural language understanding with a single task-agnostic model through generative pre-training and discriminative fine-tuning. By pre-training on a diverse corpus with long stretches of contiguous text our model acquires significant world knowledge and ability to process long-range dependencies which are then successfully transferred to solving discriminative tasks such as question answering, semantic similarity assessment, entailment determination, and text classification, improving the state of the art on 9 of the 12 datasets we study. Using unsupervised (pre-)training to boost performance on discriminative tasks has long been an important goal of Machine Learning research. Our work suggests that achieving significant performance gains is indeed possible, and offers hints as to what models (Transformers) and data sets (text with long range dependencies) work best with this approach. We hope that this will help enable new research into unsupervised learning, for both natural language understanding and other domains, further improving our understanding of how and when unsupervised learning works.

### References

[1] S. Arora, Y. Liang, and T. Ma. A simple but tough-to-beat baseline for sentence embeddings. 2016.

[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pages 153-160, 2007.

[4] L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo. The fifth pascal recognizing textual entailment challenge. In TAC, 2009.

[5] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural language inference. EMNLP, 2015.

[6] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.

[7] S. Chaturvedi, H. Peng, and D. Roth. Story comprehension for predicting what happens next. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1603-1614, 2017.

[8] D. Chen and C. Manning. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740-750, 2014.

[9] Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora question pairs. <https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs>, 2018.

[10] R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160-167. ACM, 2008.

[11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493-2537, 2011.

[12] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes. Supervised learning of universal sentence representations from natural language inference data. EMNLP, 2017.

[13] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, pages 3079-3087, 2015.

[14] W. B. Dolan and C. Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.

[15] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625-660, 2010.

[16] S. Gray, A. Radford, and K. P. Diederik. Gpu kernels for block-sparse weights. 2017.

[17] Z. He, S. Liu, M. Li, M. Zhou, L. Zhang, and H. Wang. Learning entity representation for entity disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 30-34, 2013.

[18] D. Hendrycks and K. Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. arXiv preprint arXiv:1606.08415, 2016.

[19] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 16931701, 2015.

[20] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527-1554, 2006.

[21] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. Association for Computational Linguistics (ACL), 2018.

[22] Y. Jernite, S. R. Bowman, and D. Sontag. Discourse-based objectives for fast unsupervised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017.

[23] Y. Ji and J. Eisenstein. Discriminative improvements to distributional sentence similarity. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891-896, 2013.

[24] F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuurmans. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 209-216. Association for Computational Linguistics, 2006.

[25] T. Khot, A. Sabharwal, and P. Clark. Scitail: A textual entailment dataset from science question answering. In Proceedings of AAAI, 2018.

[26] Y. Kim. Convolutional neural networks for sentence classification. EMNLP, 2014.

[27] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[28] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. Skip-thought vectors. In Advances in neural information processing systems, pages 3294-3302, 2015.

[29] N. Kitaev and D. Klein. Constituency parsing with a self-attentive encoder. ACL, 2018.

[30] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. Race: Large-scale reading comprehension dataset from examinations. EMNLP, 2017.

[31] G. Lample, L. Denoyer, and M. Ranzato. Unsupervised machine translation using monolingual corpora only. ICLR, 2018.

[32] Q. Le and T. Mikolov. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages 1188-1196, 2014.

[33] P. Liang. Semi-supervised learning for natural language. PhD thesis, Massachusetts Institute of Technology, 2005.

[34] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing long sequences. ICLR, 2018.

[35] X. Liu, K. Duh, and J. Gao. Stochastic answer networks for natural language inference. arXiv preprint arXiv:1804.07888, 2018.

[36] L. Logeswaran and H. Lee. An efficient framework for learning sentence representations. ICLR, 2018.

[37] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017.

[38] B. McCann, J. Bradbury, C. Xiong, and R. Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6297-6308, 2017.

[39] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111-3119, 2013.

[40] N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and J. Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46-51, 2017.

[41] K. Nigam, A. McCallum, and T. Mitchell. Semi-supervised text classification using em. Semi-Supervised Learning, pages 33-56, 2006.

[42] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.

[43] M. E. Peters, W. Ammar, C. Bhagavatula, and R. Power. Semi-supervised sequence tagging with bidirectional language models. ACL, 2017.

[44] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. NAACL, 2018.

[45] Y. Qi, D. S. Sachan, M. Felix, S. J. Padmanabhan, and G. Neubig. When and why are pre-trained word embeddings useful for neural machine translation? NAACL, 2018.

[46] A. Rahman and V. Ng. Resolving complex cases of definite pronouns: the winograd schema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 777-789. Association for Computational Linguistics, 2012.

[47] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. EMNLP, 2016.

[48] P. Ramachandran, P. J. Liu, and Q. V. Le. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683, 2016.

[49] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efficient learning of sparse representations with an energy-based model. In Advances in neural information processing systems, pages 1137-1144, 2007.

[50] M. Rei. Semi-supervised multitask learning for sequence labeling. ACL, 2017.

[51] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400-407, 1951.

[52] T. Rocktaschel, E. Grefenstette, K. M. Hermann, T. KoCisky, and P. Blunsom. Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664, 2015.

[53] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.

[54] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642, 2013.

[55] S. Srinivasan, R. Arora, and M. Riedl. A simple and effective approach to the story cloze test. arXiv preprint arXiv:1803.05547, 2018.

[56] S. Subramanian, A. Trischler, Y. Bengio, and C. J. Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.

[57] J. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. Proceedings of ACL-08: HLT, pages 665-673, 2008.

[58] Y. Tay, L. A. Tuan, and S. C. Hui. A compare-propagate architecture with alignment factorization for natural language inference. arXiv preprint arXiv:1801.00102, 2017.

[59] Y. Tay, L. A. Tuan, and S. C. Hui. Multi-range reasoning for machine comprehension. arXiv preprint arXiv:1803.09074, 2018.

[60] J. Tian, Z. Zhou, M. Lan, and Y. Wu. Ecnu at semeval-2017 task 1: Leverage kernel-based traditional nlp features and neural networks to build a universal model for multilingual and cross-lingual semantic textual similarity. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 191-197, 2017.

[61] Y. Tsvetkov. Opportunities and challenges in working with low-resource languages. CMU, 2017.

[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,Ł.Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010, 2017.

[63] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096-1103. ACM, 2008.

[64] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

[65] A. Warstadt, A. Singh, and S. R. Bowman. Corpus of linguistic acceptability. <http://nyu-mll.github.io/cola>, 2018.

[66] A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. NAACL, 2018.

[67] Y. Xu, J. Liu, J. Gao, Y. Shen, and X. Liu. Towards human-level machine reading comprehension: Reasoning and inference with multiple strategies. arXiv preprint arXiv:1711.04964, 2017.

[68] D. Yu, L. Deng, and G. Dahl. Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.

[69] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In CVPR, volume 1, page 6, 2017.

[70] X. Zhu. Semi-supervised learning literature survey. 2005.

[71] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.

## Language Models are Unsupervised Multitask Learners 语言模型是多任务的学习器（GPT-2）

<div align=center>Alec Radford * 1 Jeffrey Wu * 1 Rewon Child 1 David Luan 1 Dario Amodei ** 1
Ilya Sutskever ** 1</div>

> *, **Equal contribution 1OpenAI, San Francisco, California, United States. Correspondence to: Alec Radford <alec@openai.com>.

### Abstract 摘要

Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the an-swers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.

自然语言处理任务，例如问答、机器翻译、阅读理解和摘要，通常通过对任务特定数据集的监督学习来处理。我们证明，当在一个名为 WebText 的数百万网页的新数据集上接受训练时，语言模型在没有任何明确监督的情况下开始学习这些任务。当以文档加问题为条件时，语言模型生成的答案在 CoQA 数据集上达到 55 F1 - 在不使用 127,000 多个训练示例的情况下匹配或超过 4 个基线系统中的 3 个的性能。语言模型的容量对于零样本任务迁移的成功至关重要，并且增加它可以跨任务以对数线性方式提高性能。我们最大的模型 GPT-2 是一个 1.5B 【15亿个参数】参数的 Transformer，它在零样本设置中的 8 个测试语言建模数据集中的 7 个上取得了最先进的结果，但仍然不适合 WebText。模型中的示例反映了这些改进并包含连贯的文本段落。这些发现为构建语言处理系统提供了一条有前途的途径，这些系统可以从自然发生的演示中学习执行任务。

### 1. Introduction 引言

Machine learning systems now excel (in expectation) at tasks they are trained for by using a combination of large datasets, high-capacity models, and supervised learning (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei et al., 2016). Yet these systems are brittle and sensitive to slight changes in the data distribution (Recht et al., 2018) and task specification (Kirkpatrick et al., 2017). Current systems are better characterized as narrow experts rather than   competent generalists. We would like to move towards more general systems which can perform many tasks - eventually without the need to manually create and label a training dataset for each one.

机器学习系统现在通过结合使用大型数据集、高容量模型和监督学习（Krizhevsky et al., 2012）（Sutskever et al., 2014）（Amodei et 等人，2016 年）。 然而，这些系统脆弱且对数据分布（Recht 等人，2018 年）和任务规范（Kirkpatrick 等人，2017 年）的微小变化敏感。 目前的系统被更好地描述为狭隘的专家而不是称职的通才。 我们希望转向可以执行许多任务的更通用的系统——最终不需要为每个任务手动创建和标记训练数据集。

The dominant approach to creating ML systems is to collect a dataset of training examples demonstrating correct behavior for a desired task, train a system to imitate these behaviors, and then test its performance on independent and identically distributed (IID) held-out examples. This has served well to make progress on narrow experts. But the often erratic behavior of captioning models (Lake et al., 2017), reading comprehension systems (Jia & Liang, 2017), and image classifiers (Alcorn et al., 2018) on the diversity and variety of possible inputs highlights some of the shortcomings of this approach.

创建 ML 系统的主要方法是收集训练示例的数据集，这些示例展示所需任务的正确行为，训练系统模仿这些行为，然后测试其在独立同分布 (IID) 保留示例上的性能。 这对在狭义专家上取得进展起到了很好的作用。 但描述模型（Lake 等人，2017 年）、阅读理解系统（Jia & Liang，2017 年）和图像分类器（Alcorn 等人，2018 年）在可能输入的多样性和多样性方面的行为经常不稳定，这突出了一些 这种方法的缺点。

Our suspicion is that the prevalence of single task training on single domain datasets is a major contributor to the lack of generalization observed in current systems. Progress towards robust systems with current architectures is likely to require training and measuring performance on a wide range of domains and tasks. Recently, several benchmarks have been proposed such as GLUE (Wang et al., 2018) and decaNLP (McCann et al., 2018) to begin studying this.

我们怀疑单域数据集上单任务训练的普遍存在是导致当前系统中观察到的泛化能力不足的主要原因。 使用当前架构向健壮的系统迈进可能需要在广泛的领域和任务上进行培训和测量性能。 最近，已经提出了几个基准，例如 GLUE (Wang et al., 2018) 和 decaNLP (McCann et al., 2018) 开始研究这一点。

Multitask learning (Caruana, 1997) is a promising framework for improving general performance. However, multitask training in NLP is still nascent. Recent work reports modest performance improvements (Yogatama et al., 2019) and the two most ambitious efforts to date have trained on a total of 10 and 17 `(dataset, objective)` pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning perspective, each `(dataset, objective)` pair is a single training example sampled from the distribution of datasets and objectives. Current ML systems need hundreds to thousands of examples to induce functions which generalize well. This suggests that multitask training many need just as many effective training pairs to realize its promise with current approaches. It will be very difficult to continue to scale the creation of datasets and the design of objectives to the degree that may be required to brute force our way there with current techniques. This motivates exploring additional setups for performing multitask learning.

多任务学习 (Caruana, 1997) 是一个很有前途的框架，可以提高整体表现。然而，NLP 中的多任务训练仍处于初期阶段。最近的工作报告了适度的性能改进（Yogatama 等人，2019 年），迄今为止最雄心勃勃的两项工作分别训练了 10 对和 17 对“（数据集，目标）”（McCann 等人，2018 年）（Bowman等人，2018 年）。从元学习的角度来看，每个“（数据集，目标）”对都是从数据集和目标的分布中采样的单个训练示例。当前的 ML 系统需要成百上千个示例来归纳出泛化良好的函数。这表明，多任务训练需要尽可能多的有效训练对才能实现其当前方法的承诺。很难继续将数据集的创建和目标的设计扩展到可能需要用当前技术强行实现的程度。这激发了探索执行多任务学习的额外设置。【训练的时候看多个数据集，甚至有多个目标函数】

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Fig%201.png"/></div>

Figure 1. Zero-shot task performance of WebText LMs as a function of model size on many NLP tasks. Reading Comprehension results are on CoQA (Reddy et al., 2018), translation on WMT-14 Fr-En (Artetxe et al., 2017), summarization on CNN and Daily Mail (See et al.,2017), and Question Answering on Natural Questions (Kwiatkowski et al., 2019). Section 3 contains detailed descriptions of each result.
图 1.WebText LM 的零样本任务性能作为许多 NLP 任务中模型大小的函数。 阅读理解结果在 CoQA (Reddy et al., 2018) 上，翻译在 WMT-14 Fr-En (Artetxe et al., 2017) 上，摘要在 CNN 和 Daily Mail (See et al., 2017) 上，问答 关于自然问题（Kwiatkowski 等人，2019 年）。 第 3 节包含每个结果的详细描述。

The current best performing systems on language tasks utilize a combination of pre-training and supervised fine-tuning. This approach has a long history with a trend to-wards more flexible forms of transfer. First, word vectors were learned and used as inputs to task-specific architectures (Mikolov et al., 2013) (Collobert et al., 2011), then the contextual representations of recurrent networks were transferred (Dai & Le, 2015) (Peters et al., 2018), and recent work suggests that task-specific architectures are no longer necessary and transferring many self-attention blocks is sufficient (Radford et al., 2018) (Devlin et al., 2018).

目前在语言任务上表现最好的系统结合了预训练和有监督的微调。 这种方法由来已久，并有向更灵活的转移形式发展的趋势。 首先，学习词向量并将其用作特定任务架构的输入 (Mikolov et al., 2013) (Collobert et al., 2011)，然后转移循环网络的上下文表示 (Dai & Le, 2015) (Peters et al., 2018)，最近的工作表明不再需要特定于任务的架构，转移许多自注意力块就足够了 (Radford et al., 2018) (Devlin et al., 2018)。

These methods still require supervised training in order to perform a task. When only minimal or no supervised data is available, another line of work has demonstrated the promise of language models to perform specific tasks, such as commonsense reasoning (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017).

这些方法仍然需要监督训练才能执行任务。 当只有很少或没有监督数据可用时，另一行工作证明了语言模型执行特定任务的前景，例如常识推理（Schwartz 等人，2017 年）和情感分析（Radford 等人，2017 年）。

In this paper, we connect these two lines of work and con-tinue the trend of more general methods of transfer. We demonstrate language models can perform down-stream tasks in a zero-shot setting - without any parameter or archi-tecture modification. We demonstrate this approach shows potential by highlighting the ability of language models to perform a wide range of tasks in a zero-shot setting. We achieve promising, competitive, and state of the art results depending on the task.

在本文中，我们将这两条工作线联系起来，并延续了更通用的迁移方法的趋势。 我们展示了语言模型可以在零样本设置中执行下游任务——无需任何参数或架构修改。 我们通过强调语言模型在零样本设置中执行各种任务的能力来展示这种方法的潜力。 我们根据任务取得有希望的、有竞争力的和最先进的结果。

### 2. Approach 方法

At the core of our approach is language modeling. Language modeling is usually framed as unsupervised distribution estimation from a set of examples $(x_1,x_2,..., x_n)$ each composed of variable length sequences of symbols $(s_1,s_2,..., s_n)$. Since language has a natural sequential ordering, it is common to factorize the joint probabilities over symbols as the product of conditional probabilities (Jelinek & Mercer, 1980) (Bengio et al., 2003):

$$
p(x) =\prod^n_{i=1} p(s_n|s_1,..,s_{n-1}) \tag{1}
$$

This approach allows for tractable sampling from and es-timation of $p(x)$ as well as any conditionals of the form $p(s_{n-k},...,s_n|s_1,...,s_{n-k-1})$. In recent years, there have been significant improvements in the expressiveness of models that can compute these conditional probabilities, such as self-attention architectures like the Transformer (Vaswani et al., 2017).

Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distri-bution $p(output|input)$. Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model $p(output|input,task)$. This has been variously formalized in multitask and meta-learning settings. Task conditioning is often implemented at an architectural level, such as the task specific encoders and decoders in (Kaiser et al., 2017) or at an algorithmic level such as the inner and outer loop optimization framework of MAML (Finn et al., 2017). But as exemplified in McCann et al. (2018), language provides a flexible way to specify tasks, inputs, and outputs all as a sequence of symbols. For example, a translation training example can be written as the sequence `(translate to french, english text, french text)`. Likewise, a reading comprehension training example can be written as `(answer the question, document, question, answer)`. McCann et al. (2018) demonstrated it was possible to train a single model, the MQAN, to infer and perform many different tasks on examples with this type of format.

学习执行单个任务可以在概率框架中表示为估计条件分布 $p(output|input)$。由于通用系统应该能够执行许多不同的任务，即使对于相同的输入，它也应该不仅以输入为条件，而且以要执行的任务为条件。也就是说，它应该对 $p(output|input,task)$ 建模。这已经在多任务和元学习环境中得到了不同的形式化。任务调节通常在架构级别实现，例如 (Kaiser et al., 2017) 中的任务特定编码器和解码器，或在算法级别实现，例如 MAML 的内外循环优化框架 (Finn et al., 2017) ).但正如 McCann 等人所举例说明的那样。 (2018)，语言提供了一种灵活的方式来将任务、输入和输出指定为一系列符号。例如，翻译训练示例可以写成序列“（翻译成法语，英语文本，法语文本）”。同样，一个阅读理解训练的例子可以写成“（回答问题，文档，问题，答案）”。麦肯等人。 (2018) 证明可以训练单个模型 MQAN，以使用这种类型的格式在示例上推断和执行许多不同的任务。

【要做zero-shot，所以下游任务的训练输入要和预训练的输入一致，即更像自然语言，作者给出的实例如上，translate to french以及answer the question叫提示prompt】

Language modeling is also able to, in principle, learn the tasks of McCann et al. (2018) without the need for explicit supervision of which symbols are the outputs to be predicted. Since the supervised objective is the the same as the unsupervised objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective. In this slightly toy setting, the concerns with density estimation as a principled training objective discussed in (Sutskever et al., 2015) are side stepped. The problem instead becomes whether we are able to, in practice, optimize the unsupervised objective to convergence. Preliminary experiments confirmed that sufficiently large language models are able to perform multitask learning in this toy-ish setup but learning is much slower than in explicitly supervised approaches.

While it is a large step from the well-posed setup described above to the messiness of “language in the wild”, Weston (2016) argues, in the context of dialog, for the need to develop systems capable of learning from natural language directly and demonstrated a proof of concept - learning a QA task without a reward signal by using forward prediction of a teacher’s outputs. While dialog is an attractive approach, we worry it is overly restrictive. The internet contains a vast amount of information that is passively available without the need for interactive communication. Our speculation is that a language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. If a language model is able to do this it will be, in effect, performing unsupervised multitask learning. We test whether this is the case by analyzing the performance of language models in a zero-shot setting on a wide variety of tasks.

#### 2.1. Training Dataset 训练数据

Most prior work trained language models on a single domain of text, such as news articles (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.

大多数先前的工作都在单个文本域上训练语言模型，例如新闻文章 (Jozefowicz et al., 2016)、维基百科 (Merity et al., 2016) 或小说书籍 (Kiros et al., 2015)。 我们的方法促使构建尽可能大和多样化的数据集，以便在尽可能多的领域和上下文中收集任务的自然语言演示。

A promising source of diverse and nearly unlimited text is web scrapes such as Common Crawl. While these archives are many orders of magnitude larger than current language modeling datasets, they have significant data quality issues. Trinh & Le (2018) used Common Crawl in their work on commonsense reasoning but noted a large amount of doc-uments “whose content are mostly unintelligible”. We observed similar data issues in our initial experiments with Common Crawl. Trinh & Le (2018)’s best results were achieved using a small subsample of Common Crawl which included only documents most similar to their target dataset, the Winograd Schema Challenge. While this is a pragmatic approach to improve performance on a specific task, we want to avoid making assumptions about the tasks to be performed ahead of time.

多样化且几乎无限的文本的一个有前途的来源是网络抓取，例如 Common Crawl。 虽然这些档案比当前的语言建模数据集大很多数量级，但它们存在严重的数据质量问题。 Trinh & Le (2018) 在他们的常识推理工作中使用了 Common Crawl，但注意到大量文档“其内容大多难以理解”。 我们在使用 Common Crawl 的初始实验中观察到类似的数据问题。 Trinh & Le (2018) 的最佳结果是使用 Common Crawl 的一个小子样本实现的，其中仅包含与其目标数据集 Winograd Schema Challenge 最相似的文档。 虽然这是提高特定任务性能的实用方法，但我们希望避免提前对要执行的任务做出假设。【使用的reddit的网页数据作为数据集】

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%201.png"/></div>

Table 1. Examples of naturally occurring demonstrations of English to French and French to English translation found throughout the WebText training set.

------

Instead, we created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.

相反，我们创建了一个强调文档质量的新网络抓取。 为此，我们只抓取了人工策划/过滤的网页。 手动过滤完整的网络抓取将非常昂贵，因此作为起点，我们从社交媒体平台 Reddit 抓取了所有出站链接，该平台至少获得了 3 个karma。 这可以被认为是其他用户是否觉得该链接有趣、有教育意义或只是有趣的启发式指标。

The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper$^1$ content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text. We removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.

生成的数据集 WebText 包含这 4500 万个链接的文本子集。 为了从 HTML 响应中提取文本，我们结合使用了 Dragnet（Peters 和 Lecocq，2013 年）和 Newspaper$^1$ 内容提取器。 本文中呈现的所有结果都使用 WebText 的初步版本，该版本不包括 2017 年 12 月之后创建的链接，并且在重复数据删除和一些基于启发式的清理之后包含略超过 800 万个文档，总共 40 GB 的文本。 我们从 WebText 中删除了所有维基百科文档，因为它是其他数据集的通用数据源，并且由于训练数据与测试评估任务重叠，可能会使分析复杂化。

> $^1$ <https://github.com/codelucas/newspape>

#### 2.2. Input Representation

A general language model (LM) should be able to compute the probability of (and also generate) any string. Current large scale LMs include pre-processing steps such as lowercasing, tokenization, and out-of-vocabulary tokens which restrict the space of model-able strings. While processing Unicode strings as a sequence of UTF-8 bytes elegantly fulfills this requirement as exemplified in work such as Gillick et al. (2015), current byte-level LMs are not competitive with word-level LMs on large scale datasets such as the One Billion Word Benchmark (Al-Rfou et al., 2018). We observed a similar performance gap in our own attempts to train standard byte-level LMs on WebText.

Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and char-acter level inputs for infrequent symbol sequences. Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These imple-mentations would require including the full space of Unicode symbols in order to model all Unicode strings. This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added. This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. In contrast, a byte-level version of BPE only requires a base vocabulary of size 256. However, directly applying BPE to the byte sequence results in sub- optimal merges due to BPE using a greedy frequency based heuristic for building the token vocabulary. We observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens.

This input representation allows us to combine the empirical benefits of word-level LMs with the generality of byte-level approaches. Since our approach can assign a probability to any Unicode string, this allows us to evaluate our LMs on any dataset regardless of pre-processing, tokenization, or vocab size.

#### 2.3. Model

We use a Transformer (Vaswani et al., 2017) based archi-tecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of $1 / \sqrt{N}$ where $N$ is the number of residual layers. The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%202.png"/></div>

Table 2. Architecture hyperparameters for the 4 model sizes.

### 3. Experiments

We trained and benchmarked four LMs with approximately log-uniformly spaced sizes. The architectures are summarized in Table 2. The smallest model is equivalent to the original GPT, and the second smallest equivalent to the largest model from BERT (Devlin et al., 2018). Our largest model, which we call GPT-2, has over an order of magnitude more parameters than GPT. The learning rate of each model was manually tuned for the best perplexity on a 5% held-out sample of WebText. All models still underfit WebText and held-out perplexity has as of yet improved given more training time.

#### 3.1. Language Modeling

As an initial step towards zero-shot task transfer, we are interested in understanding how WebText LM’s perform at zero-shot domain transfer on the primary task they are trained for - language modeling. Since our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or ex-ponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word. We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out- of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results in Table 3 using invertible de-tokenizers which remove as many of these tokenization / pre-processing artifacts as possible. Since these de-tokenizers are invertible, we can still calculate the log probability of a dataset and they can be thought of as a simple form of domain adaptation. We observe gains of 2.5 to 5 perplexity for GPT-2 with these de-tokenizers.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%203.png"/></div>

Table 3. Zero-shot results on many datasets. No training or fine-tuning was performed for any of these results. PTB and WikiText-2 results are from (Gong et al., 2018). CBT results are from (Bajgar et al., 2016). LAMBADA accuracy result is from (Hoang et al., 2018) and LAMBADA perplexity result is from (Grave et al., 2016). Other results are from (Dai et al., 2019).

WebText LMs transfer well across domains and datasets, improving the state of the art on 7 out of the 8 datasets in a zero-shot setting. Large improvements are noticed on small datasets such as Penn Treebank and WikiText-2 which have only 1 to 2 million training tokens. Large improvements are also noticed on datasets created to measure long-term dependencies like LAMBADA (Paperno et al., 2016) and the Children’s Book Test (Hill et al., 2015). Our model is still significantly worse than prior work on the One Billion Word Benchmark (Chelba et al., 2013). This is likely due to a combination of it being both the largest dataset and having some of the most destructive pre-processing - 1BW’s sentence level shuffling removes all long-range structure.

#### 3.2. Children’s Book Test

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Fig%202.png"/></div>

Figure 2. Performance on the Children’s Book Test as a function of model capacity. Human performance are from Bajgar et al. (2016), instead of the much lower estimates from the original paper.

Figure 2. Performance on the Children’s Book Test as a function of model capacity. Human performance are from Bajgar et al. (2016), instead of the much lower estimates from the original paper.

The Children’s Book Test (CBT) (Hill et al., 2015) was created to examine the performance of LMs on different cat-egories of words: named entities, nouns, verbs, and preposi-tions. Rather than reporting perplexity as an evaluation met-ric, CBT reports accuracy on an automatically constructed cloze test where the task is to predict which of 10 possible choices for an omitted word is correct. Following the LM approach introduced in the original paper, we compute the probability of each choice and the rest of the sentence con-ditioned on this choice according to the LM, and predict the one with the highest probability. As seen in Figure 2 performance steadily improves as model size is increased and closes the majority of the gap to human performance on this test. Data overlap analysis showed one of the CBT test set books, The Jungle Book by Rudyard Kipling, is in WebText, so we report results on the validation set which has no significant overlap. GPT-2 achieves new state of the art results of 93.3% on common nouns and 89.1% on named entities. A de-tokenizer was applied to remove PTB style tokenization artifacts from CBT.

#### 3.3. LAMBADA

The LAMBADA dataset (Paperno et al., 2016) tests the ability of systems to model long-range dependencies in text. The task is to predict the final word of sentences which require at least 50 tokens of context for a human to successfully predict. GPT-2 improves the state of the art from 99.8 (Grave et al., 2016) to 8.6 perplexity and increases the accuracy of LMs on this test from 19% (Dehghani et al., 2018) to 52.66%. Investigating GPT-2’s errors showed most predictions are valid continuations of the sentence, but are not valid final words. This suggests that the LM is not using the additional useful constraint that the word must be the final of the sentence. Adding a stop-word filter as an approximation to this further increases accuracy to 63.24%, improving the overall state of the art on this task by 4%. The previous state of the art (Hoang et al., 2018) used a different restricted prediction setting where the outputs of the model were constrained to only words that appeared in the context. For GPT-2, this restriction is harmful rather than helpful since 19% of answers are not in context. We use a version of the dataset without preprocessing.

#### 3.4. Winograd Schema Challenge

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Fig%203.png"/></div>

Figure 3. Performance on the Winograd Schema Challenge as a function of model capacity.

The Winograd Schema challenge (Levesque et al., 2012) was constructed to measure the capability of a system to perform commonsense reasoning by measuring its ability to resolve ambiguities in text. Recently Trinh & Le (2018) demonstrated significant progress on this challenge using LMs, by predicting the resolution of the ambiguity with higher probability. We follow their problem formulation and visualize the performance of our models with both full and partial scoring techniques in Figure 3. GPT-2 improves state of the art accuracy by 7%, achieving 70.70%. The dataset is quite small with only 273 examples so we recommend reading Trichelair et al. (2018) to help contextualize this result.

#### 3.5. Reading Comprehension

The Conversation Question Answering dataset (CoQA) Reddy et al. (2018) consists of documents from 7 different domains paired with natural language dialogues between a question asker and a question answerer about the document. CoQA tests reading comprehension capabilities and also the ability of models to answer questions that depend on conversation history (such as “Why?”).

Greedy decoding from GPT-2 when conditioned on a doc-ument, the history of the associated conversation, and a final token A: achieves 55 F1 on the development set. This matches or exceeds the performance of 3 out of 4 baseline systems without using the 127,000+ manually collected question answer pairs those baselines were trained on. The supervised SOTA, a BERT based system (Devlin et al.,2018), is nearing the 89 F1 performance of humans. While GPT-2’s performance is exciting for a system without any su-pervised training, some inspection of its answers and errors suggests GPT-2 often uses simple retrieval based heuristics such as answer with a name from the document in response to a who question.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%204.png"/></div>

Table 4. Summarization performance as measured by ROUGE F1 metrics on the CNN and Daily Mail dataset. Bottom-Up Sum is the SOTA model from (Gehrmann et al., 2018)

#### 3.6. Summarization

We test GPT-2’s ability to perform summarization on the CNN and Daily Mail dataset (Nallapati et al., 2016). To in-duce summarization behavior we add the text TL;DR: after the article and generate 100 tokens with Top-k random sampling (Fan et al., 2018) with k = 2 which reduces repetition and encourages more abstractive summaries than greedy decoding. We use the first 3 generated sentences in these 100 tokens as the summary. While qualitatively the generations resemble summaries, as shown in Table 14, they often focus on recent content from the article or confuse specific details such as how many cars were involved in a crash or whether a logo was on a hat or shirt. On the commonly reported ROUGE 1,2,L metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article. GPT-2’s performance drops by 6.4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invoke task specific behavior in a language model with natural language.

#### 3.7. Translation

We test whether GPT-2 has begun to learn how to translate from one language to another. In order to help it infer that this is the desired task, we condition the language model on a context of example pairs of the format english sentence = french sentence and then after a final prompt of english sentence = we sample from the model with greedy decoding and use the first generated sentence as the translation. On the WMT-14 English-French test set, GPT-2 gets 5 BLEU, which is slightly worse than a word-by-word substitution with a bilingual lexicon inferred in previous work on unsupervised word translation (Conneau et al., 2017b). On the WMT-14 French-English test set, GPT-2 is able to leverage its very strong English language model to perform significantly better, achieving 11.5 BLEU. This outperforms several unsupervised machine translation baselines from (Artetxe et al., 2017) and (Lample et al., 2017) but is still much worse than the 33.5 BLEU of the current best unsupervised machine translation approach (Artetxe et al., 2019). Performance on this task was surprising to us, since we deliberately removed non-English webpages from WebText as a filtering step. In order to confirm this, we ran a byte-level language detector$^2$ on WebText which detected only 10MB of data in the French language which is approximately 500x smaller than the monolingual French corpus common in prior unsupervised machine translation research.

>$^2$ <https://github.com/CLD2Owners/cld2>

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%205.png"/></div>

Table 5. The 30 most confident answers generated by GPT-2 on the development set of Natural Questions sorted by their probability according to GPT-2. None of these questions appear in WebText according to the procedure described in Section 4.

#### 3.8. Question Answering

A potential way to test what information is contained within a language model is to evaluate how often it generates the correct answer to factoid-style questions. Previous showcas-ing of this behavior in neural systems where all information is stored in parameters such as A Neural Conversational Model (Vinyals & Le, 2015) reported qualitative results due to the lack of high-quality evaluation datasets. The recently introduced Natural Questions dataset (Kwiatkowski et al., 2019) is a promising resource to test this more quantitatively. Similar to translation, the context of the language model is seeded with example question answer pairs which helps the model infer the short answer style of the dataset. GPT-2 answers 4.1% of questions correctly when evaluated by the exact match metric commonly used on reading comprehension datasets like SQUAD.$^3$ As a comparison point, the smallest model does not exceed the 1.0% accuracy of an incredibly simple baseline which returns the most common answer for each question type (who, what, where, etc...). GPT-2 answers 5.3 times more questions correctly, suggesting that model capacity has been a major factor in the poor performance of neural systems on this kind of task as of yet. The probability GPT-2 assigns to its generated answers is well calibrated and GPT-2 has an accuracy of 63.1% on the 1% of questions it is most confident in. The 30 most confident answers generated by GPT-2 on development set questions are shown in Table 5. The performance of GPT-2 is still much, much, worse than the 30 to 50% range of open domain question answering systems which hybridize information retrieval with extractive document question answering (Alberti et al., 2019).

>$^3$ Alec, who previously thought of himself as good at random trivia, answered 17 of 100 randomly sampled examples correctly when tested in the same setting as GPT-2. He actually only got 14 right but he should have gotten those other 3

### 4. Generalization vs Memorization

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%206.png"/></div>

Table 6. Percentage of test set 8 grams overlapping with training sets.

Recent work in computer vision has shown that common image datasets contain a non-trivial amount of near-duplicate images. For instance CIFAR-10 has 3.3% overlap between train and test images (Barz & Denzler, 2019). This results in an over-reporting of the generalization performance of ma-chine learning systems. As the size of datasets increases this issue becomes increasingly likely which suggests a similar phenomena could be happening with WebText. Therefore it is important to analyze how much test data also shows up in the training data.

To study this we created Bloom filters containing 8-grams of WebText training set tokens. To improve recall, strings were normalized to contain only lower-cased alphanumeric words with a single space as a delimiter. The Bloom filters were constructed such that the false positive rate is upper bounded by $\frac{1}{10^8}$.We further verified the low false positive rate by generating 1M strings, of which zero were found by the filter.

These Bloom filters let us calculate, given a dataset, the percentage of 8-grams from that dataset that are also found in the WebText training set. Table 6 shows this overlap analysis for the test sets of common LM benchmarks. Common LM datasets’ test sets have between 1-6% overlap with WebText train, with an average of overlap of 3.2%. Somewhat surprisingly, many datasets have larger overlaps with their own training splits, with an average of 5.9% overlap.

Our approach optimizes for recall, and while manual inspec-tion of the overlaps shows many common phrases, there are many longer matches that are due to duplicated data. This is not unique to WebText. For instance, we discovered that the test set of WikiText-103 has an article which is also in the training dataset. Since there are only 60 articles in the test set there is at least an overlap of 1.6%.$^4$ Potentially more worryingly, 1BW has an overlap of nearly 13.2% with its own training set according to our procedure.

>$^4$ A significant portion of additional overlap is due to editors reusing some paragraphs across multiple articles with a shared theme such as various battles in the Korean War.

For the Winograd Schema Challenge, we found only 10 schemata which had any 8-gram overlaps with the WebText training set. Of these, 2 were spurious matches. Of the remaining 8, only 1 schema appeared in any contexts that gave away the answer.

For CoQA, about 15% of documents in the news domain are already in WebText and the model performs about 3 F1 better on these. CoQA’s development set metric reports the average performance over 5 different domains and we measure a gain of about 0.5-1.0 F1 due to overlap across the various domains. However, no actual training questions or answers are in WebText since CoQA was released after the cutoff date for links in WebText.

On LAMBADA, the average overlap is 1.2%. GPT-2 per-forms about 2 perplexity better on examples with greater than 15% overlap. Recalculating metrics when excluding all examples with any overlap shifts results from 8.6 to 8.7 perplexity and reduces accuracy from 63.2% to 62.9%. This very small change in overall results is likely due to only 1 in 200 examples having significant overlap.

Overall, our analysis suggests that data overlap between WebText training data and specific evaluation datasets pro-vides a small but consistent benefit to reported results. However, for most datasets we do not notice significantly larger overlaps than those already existing between standard training and test sets, as Table 6 highlights.

Understanding and quantifying how highly similar text im-pacts performance is an important research question. Better de-duplication techniques such as scalable fuzzy matching could also help better answer these questions. For now, we recommend the use of n-gram overlap based de-duplication as an important verification step and sanity check during the creation of training and test splits for new NLP datasets.

Another potential way of determining whether the perfor-mance of WebText LMs is attributable to memorization is inspecting their performance on their own held-out set. As shown in Figure 4, performance on both the training and test sets of WebText are similar and improve together as model size is increased. This suggests even GPT-2 is still underfitting on WebText in many ways.

GPT-2 is also able to write news articles about the discovery of talking unicorns. An example is provided in Table 13.

### 5. Related Work

A significant portion of this work measured the performance of larger language models trained on larger datasets. This is similar to the work of Jozefowicz et al. (2016) which scaled RNN based language models on the 1 Billion Word Benchmark. Bajgar et al. (2016) also previously improved results on the Children’s Book Test by creating a much larger training dataset out of Project Gutenberg to supplement the standard training dataset. Hestness et al. (2017) conducted a thorough analysis of how the performance of various deep learning models changes as a function of both model capacity and dataset size. Our experiments, while much noisier across tasks, suggest similar trends hold for sub-tasks of an objective and continue into the 1B+ parameter regime.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Fig%204.png"/></div>

Figure 4. The performance of LMs trained on WebText as a function of model size.

Interesting learned functionality in generative models has been documented before such as the cells in an RNN language model performing line-width tracking and quote/comment detection Karpathy et al. (2015). More inspirational to our work was the observation of Liu et al. (2018) that a model trained to generate Wikipedia articles also learned to translate names between languages.

Previous work has explored alternative approaches to filter-ing and constructing a large text corpus of web pages, such as the iWeb Corpus (Davies, 2018).

There has been extensive work on pre-training methods for language tasks. In addition to those mentioned in the introduction, GloVe (Pennington et al., 2014) scaled word vector representation learning to all of Common Crawl. An influential early work on deep representation learning for text was Skip-thought Vectors (Kiros et al., 2015). McCann et al. (2017) explored the use of representations derived from machine translation models and Howard & Ruder (2018) improved the RNN based fine-tuning approaches of (Dai & Le, 2015). (Conneau et al., 2017a) studied the transfer performance of representations learned by natural language inference models and (Subramanian et al., 2018) explored large-scale multitask training.

(Ramachandran et al., 2016) demonstrated that seq2seq models benefit from being initialized with pre-trained language models as encoders and decoders. More recent work has shown that LM pre-training is helpful when fine-tuned for difficult generation tasks like chit-chat dialog and dialog based question answering systems as well (Wolf et al., 2019) (Dinan et al., 2018).

### 6. Discussion

Much research has been dedicated to learning (Hill et al., 2016), understanding (Levy & Goldberg, 2014), and critically evaluating (Wieting & Kiela, 2019) the representations of both supervised and unsupervised pre-training methods. Our results suggest that unsupervised task learning is an additional promising area of research to explore. These findings potentially help explain the widespread success of pre-training techniques for down-stream NLP tasks as we show that, in the limit, one of these pre-training techniques begins to learn to perform tasks directly without the need for supervised adaption or modification.

On reading comprehension the performance of GPT-2 is competitive with supervised baselines in a zero-shot setting. However, on other tasks such as summarization, while it is qualitatively performing the task, its performance is still only rudimentary according to quantitative metrics. While suggestive as a research result, in terms of practical applica-tions, the zero-shot performance of GPT-2 is still far from useable.

We have studied the zero-shot performance of WebText LMs on many canonical NLP tasks, but there are many additional tasks that could be evaluated. There are undoubtedly many practical tasks where the performance of GPT-2 is still no better than random. Even on common tasks that we evaluated on, such as question answering and translation, language models only begin to outperform trivial baselines when they have sufficient capacity.

While zero-shot performance establishes a baseline of the potential performance of GPT-2 on many tasks, it is not clear where the ceiling is with finetuning. On some tasks, GPT-2’s fully abstractive output is a significant departure from the extractive pointer network (Vinyals et al., 2015) based outputs which are currently state of the art on many question answering and reading comprehension datasets. Given the prior success of fine-tuning GPT, we plan to investigate fine-tuning on benchmarks such as decaNLP and GLUE, especially since it is unclear whether the additional training data and capacity of GPT-2 is sufficient to overcome the inefficiencies of uni-directional representations demonstrated by BERT (Devlin et al., 2018).

### 7. Conclusion

When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets. GPT-2 zero-shots to state of the art performance on 7 out of 8 tested language modeling datasets. The diversity of tasks the model is able to perform in a zero-shot setting suggests that high-capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision. 

### Acknowledgements

Thanks to everyone who wrote the text, shared the links, and upvoted the content in WebText. Many millions of people were involved in creating the data that GPT-2 was trained on. Also thanks to all the Googlers who helped us with training infrastructure, including Zak Stone, JS Riehl, Jonathan Hseu, Russell Power, Youlong Cheng, Noam Shazeer, Solomon Boulos, Michael Banfield, Aman Gupta, Daniel Sohn, and many more. Finally thanks to the people who gave feedback on drafts of the paper: Jacob Steinhardt, Sam Bowman, Geoffrey Irving, and Madison May.

### References

Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

Alberti, C., Lee, K., and Collins, M. A bert baseline for the natural questions. arXiv preprint arXiv:1901.08634, 2019.

Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., and Nguyen, A. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. arXiv preprint arXiv:1811.11553, 2018.

Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen, G., et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In International Conference on Machine Learning,pp.173-182, 2016.

Artetxe, M., Labaka, G., Agirre, E., and Cho, K. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041, 2017.

Artetxe, M., Labaka, G., and Agirre, E. An effective approach to unsupervised machine translation. arXiv preprint arXiv:1902.01313, 2019.

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

Bajgar, O., Kadlec, R., and Kleindienst, J. Embracing data abundance: Booktest dataset for reading comprehension. arXiv preprint arXiv:1610.00956, 2016.

Barz, B. and Denzler, J. Do we train on test data? purging cifar of near-duplicates. arXiv preprint arXiv:1902.00423, 2019.

Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137-1155, 2003.

Bowman, S. R., Pavlick, E., Grave, E., Van Durme, B., Wang, A., Hula, J., Xia, P., Pappagari, R., McCoy, R. T., Patel, R., et al. Looking for elmo’s friends: Sentence-level pretraining beyond language modeling. arXiv preprint arXiv:1812.10860, 2018.

Caruana, R. Multitask learning. Machine learning, 28(1):41-75, 1997.

Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493- 2537, 2011.

Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017a.

Conneau, A., Lample, G., Ranzato, M., Denoyer, L., and Jegou, H. Word translation without parallel data. arXiv preprint arXiv:1710.04087, 2017b.

Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In Advances in neural information processing systems, pp. 30793087, 2015.

Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

Davies, M. The 14 billion word iweb corpus. <https://corpus.byu.edu/iWeb/, 2018>.

Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, 匕.Universal transformers. arXiv preprint arXiv:1807.03819, 2018.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241, 2018.

Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018.

Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.

Gehrmann, S., Deng, Y., and Rush, A. M. Bottom-up abstractive summarization. arXiv preprint arXiv:1808.10792, 2018.

Gillick, D., Brunk, C., Vinyals, O., and Subramanya, A. Multilingual language processing from bytes. arXiv preprint arXiv:1512.00103, 2015.

Gong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-Y. Frage: frequency-agnostic word representation. In Advances in Neural Information Processing Systems, pp. 1341-1352, 2018.

Grave, E., Joulin, A., and Usunier, N. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016.

He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630-645. Springer, 2016.

Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kian- inejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.

Hill, F., Bordes, A., Chopra, S., and Weston, J. The goldilocks principle: Reading children’s books with explicit memory representations. arXiv preprint arXiv:1511.02301, 2015.

Hill, F., Cho, K., and Korhonen, A. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016.

Hoang, L., Wiseman, S., and Rush, A. M. Entity tracking improves cloze-style reading comprehension. arXiv preprint arXiv:1810.02891, 2018.

Howard, J. and Ruder, S. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 328-339, 2018.

Jelinek, F. and Mercer, R. L. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands: North-Holland, May., 1980.

Jia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328, 2017.

Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.

Kaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N., Jones, L., and Uszkoreit, J. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.

Karpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.

Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska- Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017.

Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. Skip-thought vectors. In Advances in neural information processing systems, pp. 3294-3302, 2015.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.

Kwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., et al. Natural questions: a benchmark for question answering research. 2019.

Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.

Lample, G., Conneau, A., Denoyer, L., and Ranzato, M. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.

Levesque, H., Davis, E., and Morgenstern, L. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.

Levy, O. and Goldberg, Y. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pp. 2177-2185, 2014.

Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.

McCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6294-6305, 2017.

McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111-3119, 2013.

Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016.

Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.

Pennington, J., Socher, R., and Manning, C. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532-1543, 2014.

Peters, M. E. and Lecocq, D. Content extraction using diverse feature sets. In Proceedings of the 22nd International Conference on World Wide Web, pp. 89-90. ACM, 2013.

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.

Radford, A., Jozefowicz, R., and Sutskever, I. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training. 2018.

Ramachandran, P., Liu, P. J., and Le, Q. V. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683, 2016.

Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do cifar-10 classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.

Reddy, S., Chen, D., and Manning, C. D. Coqa: A conversational question answering challenge. arXiv preprint arXiv:1808.07042, 2018.

Schwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y., and Smith, N. A. Story cloze task: Uw nlp system. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pp. 52-55, 2017.

See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.

Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.

Subramanian, S., Trischler, A., Bengio, Y., and Pal, C. J. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.

Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104-3112, 2014.

Sutskever, I., Jozefowicz, R., Gregor, K., Rezende, D., Lillicrap, T., and Vinyals, O. Towards principled unsupervised learning. arXiv preprint arXiv:1511.06440, 2015.

Trichelair, P., Emami, A., Cheung, J. C. K., Trischler, A., Suleman, K., and Diaz, F. On the evaluation of common-sense reasoning in natural language understanding. arXiv preprint arXiv:1811.01778, 2018.

Trinh, T. H. and Le, Q. V. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,Ł.，and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998-6008, 2017.

Vinyals, O. and Le, Q. A neural conversational model. arXiv preprint arXiv:1506.05869, 2015.

Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing Systems, pp. 26922700, 2015.

Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

Weston, J. E. Dialog-based language learning. In Advances in Neural Information Processing Systems, pp. 829-837, 2016.

Wieting, J. and Kiela, D. No training required: Exploring random encoders for sentence classification. arXiv preprint arXiv:1901.10444, 2019.

Wolf, T., Sanh, V., Chaumond, J., and Delangue, C. Transfer- transfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019.

Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky, T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L., Dyer, C., et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019. 

### 8. Appendix A: Samples

#### 8.1. Model capacity

To complement the reported perplexity gains of bigger LMs on WebText show in Figure 4, Tables 7 through 11 show side-by-side completions of the smallest WebText LM and GPT-2 on random unseen WebText test set articles.

#### 8.2. Text Memorization

We observe some memorizing behavior in GPT-2 on longer strings that are repeated many times in the dataset such as famous quotes or speeches. For example, when conditioned on the first sentence and a half of the Gettysburg Address (which occurs approximately 40 times throughout WebText), an argmax decode from GPT-2 recovers the speech. Even when sampling without truncation, we find that the model copies the speech for awhile before drifting, albeit in a similar style. It typically drifts within 100-200 tokens, and displays widening diversity once it drifts.

To quantify how often exact memorization shows up in samples, we generated samples from GPT-2 conditioned on WebText test set articles and compared the overlap rates of GPT-2’s generations to the overlap rates of the ground-truth completions. The results of this analysis are shown below and suggest that GPT-2 repeats text from the training set less often then the baseline rate of held-out articles.

#### 8.3. Diversity

Table 12 shows multiple completions of the same random WebText test set context, showing the diversity of completions with standard sampling settings.

#### 8.4. Robustness

Table 13 shows the previously mentioned talking unicorns news article. We find the model to be capable of handling out of distribution contexts, but the quality of these samples is generally lower. 

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Fig%205.png"/></div>

Figure 5. CDF of percentage 8-gram overlap with WebText training set, for both WebText test set and samples (conditioned on WebText test set, with top-k truncated random sampling with k = 40). Most samples have less than 1% overlap, including over 30% of samples with no overlap, whereas the median for test set is 2.6% overlap.

------
<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%207.png"/></div>

Table 7. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models. Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k random sampling with k = 40 was used for generation.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%208.png"/></div>

Table 8. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models. Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-$k$ random sampling with $k = 40$ was used for generation.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%209.png"/></div>

Table 9. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models. Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k random sampling with k = 40 was used for generation.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%2010.png"/></div>

Table 10. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models. Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k random sampling with k = 40 was used for generation.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/table%2011.png"/></div>

Table 11. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models. Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k random sampling with k = 40 was used for generation.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%2012.png"/></div>

Table 12. Non-cherry-picked completions from GPT-2 generated from the same context (from WebText test). Context is 384 tokens (shown truncated), and generations are 128 tokens. Top-k random sampling with k = 40 was used for generation.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%2013.png"/></div>

Table 13. Conditional generation on an out-of-distribution context by GPT-2. Cherry pick of 10 samples generated with k = 40.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%2014.png"/></div>

Table 14. Summaries generated by GPT-2 and reference summaries on the test set of the CNN and Daily Mail dataset.

------

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-2/Table%2015.png"/></div>

Table 15. English to French and French to English translations generated by GPT-2.

------

**Context (passage and previous question/answer pairs)**

The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried the torch 137,000 km (85,000 mi) - the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics.

After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event.

Q: What was the theme
A: “one world, one dream”.

Q: What was the length of the race?
A: 137,000 km

Q: Was it larger than previous ones?
A: No

Q: Where did the race begin?
A: Olympia, Greece

Q: Is there anything notable about that place?
A: birthplace of Olympic Games

Q: Where did they go after?
A: Athens

Q: How many days was the race?
A: seven

Q: Did they visit any notable landmarks?
A: Panathinaiko Stadium

Q: And did they climb any mountains?
A:

**Model answer:** Everest
**Turker answers:** unknown, yes, Yes, yes
<div align=center>Table 16. Selected CoQA completion.</div>

------

**Context (passage and previous question/answer pairs)**

Tom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her office at work and goes shopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my fourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food, paying his health bills and his taxes, but in fact Tom is a dog.

Catherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life according to rules laid down by the government, which also provides a high level of care for its people. This level of care costs money.

People in Sweden pay taxes on everything, so aren’t surprised to find that owning a dog means more taxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most such treatment is expensive, so owners often decide to offer health and even life _ for their dog.

In Sweden dog owners must pay for any damage their dog does. A Swedish Kennel Club official explains what this means: if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay for any damage done to the car, even if your dog has been killed in the accident.

Q: How old is Catherine?
A: 54

Q: where does she live?
A:

**Model answer:** Stockholm
**Turker answers:** Sweden, Sweden, in Sweden, Sweden
<div align=center>Table 17. Selected CoQA completion.</div>

------

## Language Models are Few-Shot Learners 语言模型是小样本学习者

<div align=center>Tom B. Brown∗,Benjamin Mann∗,Nick Ryder∗,Melanie Subbiah∗</div>
<div align=center>Jared Kaplan†,Prafulla Dhariwal,Arvind Neelakantan,Pranav Shyam,Girish Sastry</div>
<div align=center>Amanda Askell,Sandhini Agarwal,Ariel Herbert-Voss,Gretchen Krueger,Tom Henighan</div>
<div align=center>Rewon Child,Aditya Ramesh,Daniel M. Ziegler,Jeffrey Wu,Clemens Winter</div>
<div align=center>Christopher Hesse,Mark Chen,Eric Sigler,Mateusz Litwin,Scott Gray</div>
<div align=center>Benjamin Chess,Jack Clark,Christopher Berner</div>
<div align=center>Sam McCandlish,Alec Radford,Ilya Sutskever,Dario Amodei</div>

### Abstract

Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine¬tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.

最近的工作表明，通过对大量文本语料库进行预训练，然后对特定任务进行微调，许多 NLP 任务和基准测试取得了实质性进展。虽然在体系结构中通常与任务无关，但此方法仍然需要特定于任务的微调数据集，其中包含数千或数万个示例。相比之下，人类通常只能通过几个例子或简单的指令来执行一项新的语言任务——这是当前的 NLP 系统在很大程度上仍然难以做到的。在这里，我们展示了扩大语言模型极大地提高了与任务无关的、少样本的性能，有时甚至可以与之前最先进的微调方法相媲美。具体来说，我们训练了 GPT-3，这是一种具有 **1750亿** 个参数的自回归语言模型，比之前的任何非稀疏语言模型多 10 倍，并在少样本设置中测试其性能。对于所有任务，GPT-3 都在没有任何梯度更新或微调的情况下应用，任务和小样本演示完全通过与模型的文本交互来指定。 GPT-3 在许多 NLP 数据集上实现了强大的性能，包括翻译、问答和完形填空任务，以及一些需要即时推理或领域适应的任务，例如解读单词，在句子，或执行 3 位数算术。同时，我们还确定了一些 GPT-3 的少样本学习仍然困难的数据集，以及一些 GPT-3 面临与大型网络语料库训练相关的方法论问题的数据集。最后，我们发现 GPT-3 可以生成人类评估者难以区分与人类撰写的文章的新闻文章样本。我们总体上讨论了这一发现和 GPT-3 的更广泛的社会影响。

>∗Equal contribution
>†Johns Hopkins University, OpenAI

### Introduction

Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].

近年来，在NLP系统中出现了预训练语言表征的趋势，并以越来越灵活和与任务无关的方式应用于下游转移。首先，单层表征是使用词向量学习的[MCCD13, PSM14]，并反馈给特定的任务架构，然后具有多层表征和上下文状态的RNN被用来形成更强大的表征[DL15, MBXS17, PNZtY18]（尽管仍然应用于特定的任务架构），最近，预训练的循环或转化器语言模型[VSP+17]已经被直接微调，完全消除对特定任务架构的需求[RNSS18, DCLT18, HR18]。

This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.

这最后一种范式在许多具有挑战性的NLP任务上取得了实质性的进展，如阅读理解、问题回答、文本嵌套等，并在新的架构和算法的基础上继续推进[RSR+19, LOG+19, YDY+19, LCG+19] 。然而，这种方法的一个主要限制是，虽然架构是任务无关的，但仍然需要特定任务的数据集和特定任务的微调：要在一个期望的任务上实现强大的性能，通常需要在数千到数十万个特定于该任务的例子的数据集上进行微调。消除这一限制是可取的，原因有几个。

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%201.1.png"/></div>

Figure 1.1: **Language model meta-learning.** During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence.\
**语言模型元学习** 在无监督的预训练过程中，一个语言模型发展出一套广泛的技能和模式识别能力。然后，它在推理时间使用这些能力来迅速适应或识别所需的任务。我们使用术语 "上下文学习 "来描述这个过程的内部循环，它发生在每个序列的前向传递中。本图中的序列并不是为了代表模型在预训练期间看到的数据，而是为了表明有时在一个序列中会有重复的子任务。
【如何将语言模型类比成一个meta-learning】

First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.

首先，从实用的角度来看，每项新任务都需要一个大型的标记实例数据集，这限制了语言模型的适用性。存在着非常广泛的可能有用的语言任务，包括从纠正语法，到生成抽象概念的例子，再到批评一个短篇故事。对于许多这样的任务来说，收集一个大型的监督训练数据集是很困难的，尤其是当这个过程必须为每一个新的任务重复时。

Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it [YdC+19, MPL19]. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19].

其次，利用训练数据中的虚假关联的可能性从根本上说是随着模型的表现力和训练分布的狭窄程度而增长的。这可能会给预训练加微调范式带来问题，即模型被设计得很大，以便在预训练期间吸收信息，但随后在非常狭窄的任务分布中进行微调。例如，[HLW+20]观察到较大的模型不一定在分布外有更好的泛化。有证据表明，在这种范式下实现的泛化可能很差，因为模型对训练分布过于具体，在分布外的泛化效果不好[YdC+19, MPL19]。因此，微调模型在特定基准上的表现，即使名义上是人类水平，也可能夸大了基础任务的实际表现[GSL+18, NK19]。【微调上面的效果好也不能说明模型泛化性好，只能说明模型有可能过拟合了预训练的数据，而这些数据在下游任务中也有出现 】

Third, humans do not require large supervised datasets to learn most language tasks - a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages - it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.

One potential route towards addressing these issues is meta-learning $^1$ — which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.

解决这些问题的一个潜在途径是**元学习**【训练一个很大的模型，泛化性不错】$^1$--在语言模型的背景下，这意味着模型在训练时发展出一套广泛的技能和模式识别能力，然后在推理时使用这些能力来快速适应或识别所需的任务（如图1.1所示）。最近的工作[RWC+19]试图通过我们所说的 "**语境学习**"【即使告诉我一些训练样本，也不更新模型的权重】来做到这一点，使用预训练的语言模型的文本输入作为任务规范的形式：该模型以自然语言指令和/或任务的几个示范为条件，然后期望通过预测接下来的内容来完成任务的进一步实例。

>$^1$ In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous: the method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning” to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training — this is an important issue which we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer loop structure. \
在语言模型的背景下，这有时被称为 "零样本迁移"，但这个术语有可能是模糊的：该方法是 "零样本 "的，因为没有进行梯度更新，但它往往涉及到向模型提供推理时间的演示，所以不是真正的从零例子中学习。为了避免这种混淆，我们使用术语 "元学习 "来捕捉一般方法的内环/外环结构，并使用术语 "in context-learning "来指称元学习的内环。我们进一步将描述专门化为 "零次"、"一次 "或 "几次"，这取决于在推理时提供多少个演示。这些术语旨在对模型在推理时是从头开始学习新任务还是仅仅识别训练时看到的模式这一问题保持不可知--这是一个重要的问题，我们将在本文后面讨论，但 "元学习 "旨在包括这两种可能性，并简单地描述内-外循环结构。

While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning - for example [RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.

Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and finally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.

语言建模的另一个最新趋势可能提供了一个前进的方向。近年来，转化器语言模型的容量大幅增加，从1亿个参数[RNSS18]，到3亿个参数[DCLT18]，到15亿个参数[RWC+19]，到80亿个参数[SPP+19]，110亿个参数[RSR+19]，最后到170亿个参数[Tur20]。每一次增加都带来了文本合成和/或下游NLP任务的改进，而且有证据表明，与许多下游任务有很好关联的对数损失，随着规模的扩大也有平滑的改进趋势[KMH+20]。由于上下文学习涉及在模型的参数内吸收许多技能和任务，因此，上下文学习能力可能会随着规模的扩大而显示出类似的强劲收益。

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%201.3.png"/></div>

**Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks** While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.

In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will fit into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.

在本文中，我们通过训练一个1750亿个参数的自回归语言模型（我们称之为GPT-3）来测试这一假设，并测量其上下文学习能力。具体来说，我们在二十多个NLP数据集上评估GPT-3，以及几个旨在测试快速适应不太可能直接包含在训练集中的任务的新任务。对于每个任务，我们在3种条件下评估GPT-3：(a) "少数次学习"，或在上下文中学习，我们允许尽可能多的演示，以适应模型的上下文窗口（通常是10至100），(b) "一次学习"，我们只允许一次演示，和(c) "零样本"学习，不允许演示，只给模型一个自然语言的指令。原则上，GPT-3也可以在传统的微调设置中进行评估，但我们将此留给未来的工作。

Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.

Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.

GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.

At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.

A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).

We also undertake a systematic study of “data contamination” - a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.

In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.

Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.

The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.

### Aproach 方法

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%202.1.png"/></div>

Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. The panels above show four methods for performing a task with a language model – fine-tuning is the traditional method, whereas zero-, one-,and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G.

#### 2.1 Model and Architectures

We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.

我们使用与GPT-2[RWC+19]相同的模型和架构，包括其中描述的修改后的初始化、预规范化和可逆标记化，不同的是我们在转化器的层中使用交替的密集和局部带状的稀疏注意模式，类似于Sparse Transformer[CGRS19]。为了研究ML性能对模型大小的依赖性，我们训练了8种不同大小的模型，范围从1.25亿个参数到1750亿个参数的三个数量级，最后一个是我们称之为GPT-3的模型。以前的工作[KMH+20]表明，在有足够训练数据的情况下，验证损失的缩放应该是一个平滑的幂律，作为规模的函数；训练许多不同规模的模型，使我们能够测试这个假设，无论是验证损失还是下游的语言任务。

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Table%202.1.png"/></div>

Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens.

Table 2.1 shows the sizes and architectures of our 8 models. Here $n_{\mathbf{params}}$ is the total number of trainable parameters, $n_{\mathbf{layers}}$ is the total number of layers, $d_{model}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\mathbf{ff}} = 4 * d_{model}$), and dhead is the dimension of each attention head. All models use a context window of $n_{ctx}$ = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU’s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.

### 2.2 Training Dataset

Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset$^2$ [RSR+19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.

语言模型的数据集已经迅速扩大，最终在Common Crawl数据集$^2$ [RSR+19]中构成了近一万亿字。这个数据集的规模足以训练我们最大的模型，而不需要在同一个序列上更新两次。然而，我们发现，未经过滤或轻度过滤的Common Crawl版本的质量往往低于更精心策划的数据集。因此，我们采取了3个步骤来提高我们的数据集的平均质量：（1）我们下载并过滤了一个基于与一系列高质量参考语料库相似性的CommonCrawl版本，（2）我们在数据集内部和之间进行了文件级别的模糊重复数据删除，以防止冗余并保持我们所保留的验证集的完整性，作为对过度拟合的精确测量，以及（3）我们还将已知的高质量参考语料库加入训练组合，以增强CommonCrawl并提高其多样性。

【CommonCrawl作为负例，Reddit作为正例，然后做个分类器，然后把CommonCrawl中比较好的文章筛出来，此外，还将相似的文章做筛选，用的算法是lsh,也加入了一些已知的高质量的数据集】

> $^2$ <https://commoncrawl.org/the-data/>

Details of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected by scraping links over a longer period of time, and first described in [KMH+20], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.

Table 2.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%202.2.png"/></div>

Figure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models [KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D.

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Table%202.2.png"/></div>

Table 2.2: Datasets used to train GPT-3. “Weight in training mix” refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once.

A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.

#### 2.3 Training Process

As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.

正如在[KMH+20, MKAT18]中发现的那样，较大的模型通常可以使用较大的批次规模，但需要较小的学习率。我们在训练过程中测量梯度噪声规模，并使用它来指导我们对批次大小的选择[MKAT18]。表2.1显示了我们使用的参数设置。为了在不耗尽内存的情况下训练更大的模型，我们在每个矩阵乘法内使用模型并行，并在网络的各层使用模型并行。所有模型都是在微软提供的高带宽集群的一部分的V100 GPU上训练的。训练过程和超参数设置的细节在附录B中描述。

#### 2.4 Evaluation

For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.

$K$ can be any value from 0 to the maximum amount allowed by the model’s context window, which is $n_{ctx} = 2048$ for all models and typically fits 10 to 100 examples. Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for $K = 0$, instead of) demonstrations.

$K$ 可以是从 0 到模型上下文窗口允许的最大数量之间的任何值，对于所有模型来说都是 $n_{ctx} = 2048$，通常适合 10 到 100 个示例。 较大的 $K$ 值通常但并不总是更好，因此当有单独的开发和测试集可用时，我们在开发集上试验几个 $K$ 值，然后在测试集上运行最佳值。 对于某些任务（请参阅附录 G），除了（或 $K = 0$，而不是）演示之外，我们还使用自然语言提示。

On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\frac{P (\rm {completion|contexta})}{P (\rm{completion|answer\_context})}$, where answer context is the string `"Answer: "` or `"A: "` and is used to prompt that the completion should be an answer but is otherwise generic.

对于涉及从多个选项（多项选择）中选择一个正确完成的任务，我们提供 $K$ 个上下文示例加上正确完成，然后是一个仅上下文示例，并比较每个完成的 LM 可能性。 对于大多数任务，我们比较每个标记的可能性（对长度进行归一化），但是在少数数据集（ARC、OpenBookQA 和 RACE）上，我们通过每个标记的无条件概率归一化获得了在开发集上测量的额外好处 完成，通过计算 $\frac{P (\rm {completion|contexta})}{P (\rm{completion|answer\_context})}$，其中答案上下文是字符串 `"Answer: "` 或 `" A: "` and 用于提示完成应该是一个答案，但在其他方面是通用的。

On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details.

在涉及二元分类的任务上，我们给选项赋予更多语义上有意义的名称（例如“True”或“False”，而不是 0 或 1），然后将任务视为多项选择； 我们有时也会将任务构建为类似于 [RSR+19] 完成的任务（有关详细信息，请参阅附录 G）。

On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.

在具有自由形式完成的任务上，我们使用与 [RSR+19] 相同参数的集束搜索：集束宽度为 4，长度惩罚 = 0.6。 我们使用 F1 相似性分数、BLEU 或精确匹配对模型进行评分，具体取决于手头数据集的标准。

Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.

Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.

### 3 Results

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%203.1.png"/></div>

Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts. \
图 3.1：通过计算平滑扩展性能。 性能（根据交叉熵验证损失来衡量）遵循幂律趋势与用于训练的计算量。 在 [KMH+20] 中观察到的幂律行为持续了另外两个数量级，与预测曲线的偏差很小。 对于此图，我们从计算和参数计数中排除了嵌入参数。【pow low分布，随着计算量指数的增加，损失是线性下降】

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%203.2.png"/></div>

Figure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text. \
图 3.2：在 LAMBADA 上，语言模型的 few-shot 能力极大地提高了准确性。 GPT-3 2.7B 在此设置中优于 SOTA 17B 参数 Turing-NLG [Tur20]，GPT-3 175B 领先 state of the art 18%。 注意零样本使用与文本中描述的单样本和少样本不同的格式。

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Table%203.3.png"/></div>

**Table 3.3: Results on three Open-Domain QA tasks.** GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server. \
**表 3.3：三个开放域 QA 任务的结果。** GPT-3 显示在少样本、单样本和零样本设置中，与之前的封闭书本和开放域设置的 SOTA 结果相比。 TriviaQA few-shot 结果在 wiki 拆分测试服务器上进行评估。【T5:编码器和解码器都有，来自Google】

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%203.3.png"/></div>

Figure 3.3: On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG [LPP+20] \
图 3.3：在 TriviaQA 上，GPT3 的性能随着模型大小的增加而平稳增长，这表明语言模型会随着容量的增加而继续吸收知识。 One-shot 和 few-shot 性能显着优于 zero-shot 行为，匹配并超过 SOTA 微调开放域模型 RAG [LPP+20] 的性能

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%203.14.png"/></div>

Figure 3.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy: 12%). \
GPT-3 生成的新闻文章是人类最难与人类书面文章区分开来的（准确率：12%）。

<div align=center><img src="https://raw.githubusercontent.com/jiugexuan/image-repository/main/Papers/GPT-3/Fig%203.16.png"/></div>

Figure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3’s completions, plain text is human prompts. In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 other than the conditioning shown here.

### 5 Limitations

GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work.

GPT-3和我们对它的分析有一些限制。下面我们描述其中的一些，并提出未来工作的方向。

First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA [BZB+19]) that test this domain. Specifically GPT-3 has difficulty with questions of the type “If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks.

首先，尽管GPT-3在数量和质量上都有很大的改进，特别是与它的直接前身GPT-2相比，它在文本合成和几个NLP任务中仍然有明显的弱点。在文本合成方面，虽然整体质量很高，但GPT-3样本有时仍会在文档层面上重复自己的语义，在足够长的段落中开始失去连贯性，自相矛盾，并偶尔包含非连续的句子或段落。我们将发布一个由500个未经整理的无条件样本组成的集合，以帮助更好地了解GPT-3在文本合成方面的限制和优势。在离散语言任务领域，我们非正式地注意到，GPT-3似乎在 "常识性物理 "方面有特殊困难，尽管在一些测试这一领域的数据集（如PIQA [BZB+19]）上表现良好。具体来说，GPT-3对 "如果我把奶酪放进冰箱，它会融化吗？"这类问题有困难。从数量上看，GPT-3的语境学习性能在我们的一套基准上有一些明显的差距，如第3节所述，特别是在一些 "比较 "任务上，如确定两个词在一个句子中的使用方式是否相同，或一个句子是否意味着另一个句子（分别为WIC和ANLI），以及在阅读理解任务的一个子集上，它的表现比机会好不了多少。鉴于GPT-3在许多其他任务上的强大的几率表现，这一点尤其引人注目。【在文本生成上仍有问题，如果生成长文本会出现问题，难以实现写小说】

GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”.

GPT-3有几个结构和算法上的限制，这可能是上述一些问题的原因。我们专注于探索自回归语言模型中的语境学习行为，因为用这种模型类进行采样和计算似然是很简单的。因此，我们的实验不包括任何双向架构或其他训练目标，如去噪。这与最近的许多文献有明显的不同，这些文献记录了在使用这些方法时比标准语言模型有更好的微调性能[RSR+19]。因此，我们的设计决定的代价是，在经验上受益于双向性的任务上，性能可能会更差。这可能包括填空任务、涉及回顾和比较两段内容的任务，或者需要重读或仔细考虑一个长的段落然后产生一个非常短的答案的任务。这可能是对GPT-3在一些任务中落后的几项表现的解释，如WIC（涉及比较一个词在两个句子中的使用）、ANLI（涉及比较两个句子，看一个是否暗示另一个），以及一些阅读理解任务（如QuAC和RACE）。根据过去的文献，我们还猜想，一个大型的双向模型在微调方面会比GPT-3更强。以GPT-3的规模制作一个双向模型，和/或尝试使双向模型适用于少数或零样本学习，是未来研究的一个有希望的方向，可以帮助实现 "两个世界的最佳"。【不能像Bert一样，从后往前看】

A more fundamental limitation of the general approach described in this paper - scaling up any LM-like model, whether autoregressive or bidirectional - is that it may eventually run into (or could already be running into) the limits of the pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. [RRS20] demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world [BHT+ 20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans [ZSW+19a], fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world [CLY+ 19].

本文中描述的一般方法的一个更基本的限制 - 扩展任何类似 LM 的模型，无论是自回归还是双向 - 是它最终可能会遇到（或可能已经遇到）预训练目标的限制。我们当前的目标对每个标记均等加权，并且缺乏关于预测什么最重要以及什么不那么重要的概念。 [RRS20] 展示了为感兴趣的实体定制预测的好处。此外，对于自我监督的目标，任务规范依赖于将所需任务强制转化为预测问题，而最终，有用的语言系统（例如虚拟助手）可能更好地被认为是采取目标导向的行动，而不仅仅是做出预测。最后，大型预训练语言模型不基于其他经验领域，例如视频或现实世界的物理交互，因此缺乏大量关于世界的上下文 [BHT+ 20]。由于所有这些原因，扩展纯自监督预测可能会达到极限，并且可能需要使用不同的方法进行扩充。在这方面有前途的未来方向可能包括从人类那里学习目标函数 [ZSW+19a]，通过强化学习进行微调，或添加其他模式（例如图像）以提供基础和更好的世界模型 [CLY+19]。
【1.无法分析语句中的重点，忽略掉虚词等没有用的词语2.没有见过其他数据类型】

Another limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements.

语言模型普遍存在的另一个限制是预训练期间的采样效率低。虽然GPT-3在测试时的取样效率方面迈出了一步，更接近于人类的取样效率（单次或零次），但它在预训练期间看到的文本仍然比人类在其一生中看到的多得多[Lin20]。提高预训练的样本效率是未来工作的一个重要方向，它可能来自于在物理世界的基础上提供额外的信息，或者来自于算法的改进。【样本有效性不够】

A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research.

与GPT-3中的几率学习相关的一个限制，或者至少是不确定性，是关于几率学习在推理时是否真的 "从头开始 "学习新的任务，或者它是否只是识别和鉴定在训练时学到的任务。这些可能性存在于一个谱系中，从训练集中的演示与测试时的演示完全相同的分布，到识别相同的任务但以不同的形式，到适应一般任务（如QA）的特定风格，到完全从头学习一项技能。GPT-3在这个频谱上的位置也可能因任务而异。合成任务，如填词或定义无意义的词，似乎特别有可能从头学起，而翻译显然必须在预训练中学习，尽管可能来自与测试数据在组织和风格上非常不同的数据。最终，我们甚至不清楚人类从零开始学习什么，还是从先前的演示中学习。即使是在预训练中组织不同的示范，并在测试时识别它们，对语言模型来说也是一种进步，但尽管如此，准确地理解少数几次的学习是未来研究的一个重要的未探索的方向。【不确定是否是从头开始学习，即通过样本学习问题还是是从训练样本找出相关的任务】

A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation [HVD15] of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general [LHCG19a] but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size.

无论目标函数或算法如何，与 GPT-3 规模的模型相关的一个限制是它们既昂贵又不方便进行推理，这可能对当前形式的这种规模模型的实际适用性提出挑战 . 解决这个问题的一个可能的未来方向是将大型模型提炼 [HVD15] 到特定任务的可管理大小。 像 GPT-3 这样的大型模型包含非常广泛的技能，其中大部分不是特定任务所需要的，这表明原则上积极的蒸馏是可能的。 蒸馏在一般情况下得到了很好的探索 [LHCG19a]，但尚未在千亿参数的规模上进行过尝试； 将其应用于这种尺寸的模型可能会带来新的挑战和机遇。【训练很贵】

Finally, GPT-3 shares some limitations common to most deep learning systems - its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue - biases in the data that may lead the model to generate stereotyped or prejudiced content - is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6).

最后，GPT-3与大多数深度学习系统有一些共同的局限性--它的决定不容易解释，它对新输入的预测不一定有很好的校准，正如在标准基准上观察到的比人类高得多的性能差异，并且它保留了它所训练的数据的偏见。最后一个问题--数据中的偏见可能导致模型产生定型或偏见的内容--从社会的角度来看是特别值得关注的，并将在下一节关于更广泛的影响（第6节）中与其他问题一起讨论。【无法解释】

### 6 Broader Impacts 更广泛的影响

Language models have a wide range of beneficial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models.

Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also briefly discuss issues of energy efficiency (Section 6.3).

#### 6.1 Misuse of Language Models

Malicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact [Ros12]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.

语言模型的恶意使用可能有点难以预料，因为它们通常涉及在非常不同的环境中或出于与研究人员预期不同的目的重新利用语言模型。 为了帮助解决这个问题，我们可以从传统的安全风险评估框架的角度来思考，它概述了关键步骤，例如识别威胁和潜在影响、评估可能性以及将风险确定为可能性和影响的组合 [Ros12]。 我们讨论了三个因素：潜在的滥用应用程序、威胁参与者和外部激励结构。

### 8 Conclusion 结论

We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of state-of-the-art fine-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks defined on-the-fly. We documented roughly predictable trends of scaling in performance without using fine-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.

我们展示了一个 1750 亿参数的语言模型，它在零样本、单样本和少样本设置中的许多 NLP 任务和基准测试中表现出强大的性能，在某些情况下几乎与最先进的精细模型的性能相匹配 调优系统，以及在动态定义的任务中生成高质量样本和强大的定性性能。 我们记录了在不使用微调的情况下大致可预测的性能扩展趋势。 我们还讨论了此类模型的社会影响。 尽管有许多限制和弱点，但这些结果表明，非常大的语言模型可能是开发适应性强的通用语言系统的重要组成部分。
